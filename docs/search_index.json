[["index.html", "Estimación de la tasa de informalidad en República Dominicana empleando modelos de área con transformación arcoseno Estandarizar bases de datos Prefacio", " Estimación de la tasa de informalidad en República Dominicana empleando modelos de área con transformación arcoseno Estandarizar bases de datos Andrés Gutiérrez CEPAL - Unidad de Estadísticas Sociales Stalyn Guerrero CEPAL - Unidad de Estadísticas Sociales 2023-03-08 Prefacio La versión online de este documento está licenciada bajo una Licencia Internacional de Creative Commons para compartir con atribución no comercial 4.0. El diagrama del flujo de trabajo seguido para la implentación de la metodología es la siguiente. "],["uso-de-imágenes-satelitales-y-sae.html", "Capítulo 1 Uso de imágenes satelitales y SAE", " Capítulo 1 Uso de imágenes satelitales y SAE Uno de los artículo pioneros de estimación de áreas pequeñas fue el artículo de Singh, R, et. al. (2002) el cual abordó la estimación del rendimiento de cultivos para los tehsil (unidad subadministrativa) del distriyo Rohtak district en Haryana (India). Las imágenes raster representan el mundo mediante un conjunto de celdas contiguas igualmente espaciadas conocidas como pixeles, estas imágenes tienen información como un sistema de información geográfico, Un sistema de referencia de coordenadas. Las imágenes almacenan un identificador, un valor en cada pixel (o un vector con diferentes valores) y cada celda tiene asociada una escala de colores. Las imágenes pueden obtenerse crudas y procesadas, estas primeras contienen solamente las capas de colores, las segundas contienen también valores que han sido procesados en cada celda (índices de vegetación, intensidad lumínica, tipo de vegetación). La información cruda puede utilizarse para entrenar características que se desean entrenar (carreteras, tipo de cultivo, bosque / no bosque), afortunadamente en Google Earth Engine encontramos muchos indicadores procesadas asociadas a un pixel. Estos indicadores pueden agregarse a nivel de un área geográfica. "],["fuentes-de-datos-de-imágenes-satelitales.html", "1.1 Fuentes de datos de imágenes satelitales", " 1.1 Fuentes de datos de imágenes satelitales Algunas de las principales fuentes de imágenes satelitales son: http://earthexplorer.usgs.gov/ https://lpdaacsvc.cr.usgs.gov/appeears/ https://search.earthdata.nasa.gov/search https://scihub.coGTMnicus.eu/ https://aws.amazon.com/public-data-sets/landsat/ Sin embargo la mayor parte de estas fuentes están centralizadas en Google Earth Engine que GTMmite buscar fuentes de datos provenientes de imágenes satelitales. GEE se puede manejar porm medio de APIS en diferentes lenguajes de programación: Javascript (por defecto), Python y R (paquete rgee). "],["google-earth-eninge.html", "1.2 Google Earth Eninge", " 1.2 Google Earth Eninge Crear una cuenta en link, una vez que se ingrese a la cuenta puede buscarse los conjuntos de datos de interés: Una vez se busque el conjunto de datos se puede abrir un editor de código brindado por google en Javascript. Copiar y pegar la sintaxis que brinda el buscador de conjunto de datos para visualizar la imagen raster y disponer de sentencias que GTMmitan la obtención del conjunto de datos de interés posteriormente en R "],["instalación-de-rgee.html", "1.3 Instalación de rgee", " 1.3 Instalación de rgee Descargar e instalar anaconda o conda. (https://www.anaconda.com/products/individual) Abrir Anaconda prompt y configurar ambiente de trabajo (ambiente python rgee_py) con las siguientes sentencias: conda create -n rgee_py python=3.9 activate rgee_py pip install google-api-python-client pip install earthengine-api pip install numpy Listar los ambientes de Python disponibles en anaconda prompt conda env list Una vez identificado la ruta del ambiente ambiente rgee_py definirla en R (no se debe olvidar cambiar \\ por /). Instalar reticulate y rgee, cargar paquetes para procesamiento espacial y configurar el ambiente de trabajo como sigue: library(reticulate) # Conexión con Python library(rgee) # Conexión con Google Earth Engine library(sf) # Paquete para manejar datos geográficos library(dplyr) # Paquete para procesamiento de datos rgee_environment_dir = &quot;C://Users//guerr//.conda//envs//rgee_py&quot; # Configurar python (Algunas veces no es detectado y se debe reiniciar R) reticulate::use_python(rgee_environment_dir, required=T) rgee::ee_install_set_pyenv(py_path = rgee_environment_dir, py_env = &quot;rgee_py&quot;) Sys.setenv(RETICULATE_PYTHON = rgee_environment_dir) Sys.setenv(EARTHENGINE_PYTHON = rgee_environment_dir) Una vez configurado el ambiente puede iniciarlizarse una sesión de Google Earth Engine como sigue: rgee::ee_Initialize(drive = T) Notas: Se debe inicializar cada sesión con el comando rgee::ee_Initialize(drive = T). Los comandos de javascript que invoquen métodos con “.” se sustituyen por signo peso ($), por ejemplo: ee.ImageCollection().filterDate() # Javascript ee$ImageCollection()$filterDate() # R "],["descarga-de-imágenes-satelitales-y-validación-de-visual.html", "Capítulo 2 Descarga de imágenes satelitales y validación de visual", " Capítulo 2 Descarga de imágenes satelitales y validación de visual La técnica de estimación de áreas pequeñas de Fay-Herriot, se basa en la combinación de información de una muestra de la población y de una fuente auxiliar, como las imágenes satelitales. Esta técnica se utiliza para mejorar la precisión de las estimaciones de áreas pequeñas, especialmente cuando la muestra de la población es limitada o no es representativa de la población completa. Las imágenes satelitales pueden proporcionar información sobre características geográficas y ambientales de una zona, como la densidad de vegetación, el tipo de suelo, el uso del suelo, la topografía, entre otros. Esta información se puede utilizar para mejorar las estimaciones de las características de la población que se está estudiando. La técnica Fay-Herriot es muy útil en áreas pequeñas donde la variabilidad de los datos es alta y la muestra es limitada. Las imágenes satelitales pueden ayudar a compensar la falta de información y proporcionar una fuente adicional de datos para mejorar las estimaciones. "],["librerias-requeridas.html", "2.1 Librerias requeridas", " 2.1 Librerias requeridas Para llevar a cabo la descarga de imágenes satelitales, es necesario realizar un proceso previo que involucra la interconexión entre tres herramientas: R, Python y Google Earth Engine. Una vez configuradas estas herramientas, es posible obtener información desde Google Earth Engine utilizando R. Es importante destacar que, para llevar a cabo esta tarea, se requiere del uso de diversas librerías que se encargan de procesar y descargar la información necesaria. A continuación, se presentan las librerías imprescindibles para llevar a cabo este proceso de manera eficiente. library(tidyverse) # Procesamiento de bases de datos. library(magrittr) library(reticulate) # Conexión con Python library(rgee) # Conexión con Google Earth Engine library(sf) # Paquete para manejar datos geográficos library(tmap) library(sp) library(concaveman) # Colapso de polígonos dentro de un archivo shapefile. library(geojsonio) # Requerido por rgee "],["inicializando-google-earth-engine-desde-r.html", "2.2 Inicializando Google Earth Engine desde R", " 2.2 Inicializando Google Earth Engine desde R El proceso de la descarga de información de Google Earth Engine comienza con el inicio de sesión en Google Earth Engine desde R, para ello se debe ejecutar el siguiente bloque de código: rgee_environment_dir = &quot;C://Users//sguerrero//Anaconda3//envs//rgee_py//python.exe&quot; reticulate::use_python(rgee_environment_dir, required = T) donde “C://Users//sguerrero//Anaconda3//envs//rgee_py//python.exe” debe ser sustituido por la ruta donde creo el espacio de trabajo para Python. Ahora, a ejecutar la siguiente linea de código seleccione la opción 2, para agilizar el proceso. rgee::ee_install_set_pyenv(py_path = rgee_environment_dir, py_env = &quot;rgee_py&quot;) Finalmente, al ejecutar el comando ‘rgee::ee_Initialize(drive = T)’, se inicia la conexión entre Google Earth Engine y R, lo que permite la descarga de imágenes satelitales. Con este paso completado, se está preparado para comenzar a trabajar con los datos satelitales disponibles en la plataforma. Sys.setenv(RETICULATE_PYTHON = rgee_environment_dir) Sys.setenv(EARTHENGINE_PYTHON = rgee_environment_dir) rgee::ee_Initialize(drive = T) Sí todo ha salido bien, debería ver un resultado como el que muestra a continuación. Figura 2.1: Inicializando Google Earth Engine "],["descarga-de-información-del-satélite-a-nivel-distrito..html", "2.3 Descarga de información del Satélite a nivel distrito.", " 2.3 Descarga de información del Satélite a nivel distrito. A continuación, se explicará el proceso de descarga de información satelital a nivel de distrito. Para comenzar, es necesario contar con un archivo Shapefile, el cual es un formato utilizado para almacenar información geoespacial. Este tipo de archivo contiene datos vectoriales tales como puntos, líneas y polígonos, junto con sus atributos asociados. En el siguiente bloque de código esta dividido en dos partes. La primera parte del código se encarga de leer el Shapefile que contiene los datos de los distritos, utilizando la función read_sf del paquete sf. El archivo se almacena en la variable poligonos_distrito. La segunda parte del código extrae los parámetros de una imagen satelital mediante el uso del paquete rgee. La imagen es una colección de imágenes nocturnas llamada NOAA/DMSP-OLS/NIGHTTIME_LIGHTS. Se filtran las imágenes de un periodo particular (“2013-01-01” hasta “2014-01-01”) y se selecciona una banda específica de la imagen (“stable_lights”). Luego, se utilizan las imágenes seleccionadas para generar una nueva banda de la imagen, que se almacena en la variable “luces”. # Lectura del shapefile de los distritos. poligonos_distrito &lt;- read_sf( &quot;shapefiles2010/DMCenso2010.shp&quot;) # Extrayendo los parámetros de la imagen satelital. luces &lt;- ee$ImageCollection(&quot;NOAA/DMSP-OLS/NIGHTTIME_LIGHTS&quot;) %&gt;% ee$ImageCollection$filterDate(&quot;2013-01-01&quot;, &quot;2014-01-01&quot;) %&gt;% ee$ImageCollection$map(function(x) x$select(&quot;stable_lights&quot;)) %&gt;% ee$ImageCollection$toBands() El proceso de obtención de la medida de resumen (media) de los pixeles de una imagen se realiza distrito por distrito. Para homologar los códigos de Java en R, se utiliza una sintaxis específica. Cabe mencionar que cada píxel de una imagen representa una porción de la misma, y contiene información sobre el color y el brillo de dicha porción. Por lo tanto, la obtención de la medida de resumen de los pixeles resulta fundamental para el análisis de la imagen en cuestión. DOM_luces_distrito &lt;- map(unique(poligonos_distrito$ENLACE), ~tryCatch( ee_extract( x = luces, y = poligonos_distrito[&quot;ENLACE&quot;] %&gt;% filter(ENLACE == .x), ee$Reducer$mean(), sf = FALSE ) %&gt;% mutate(ENLACE = .x), error = function(e)data.frame(ENLACE = .x))) El código presentado utiliza la librería purrr de R para extraer la media de los valores que toma cada pixel de la imagen satelital que hacen referencia a la iluminación nocturna para cada distrito presente en una lista de polígonos. La función map aplica una función a cada distrito presente en poligonos_distrito que extrae la media de brillo de stable_lights para el polígono que corresponde al distrito en cuestión. La función tryCatch maneja posibles errores en la extracción de la media de brillo para cada distrito y retorna un data.frame con los valores extraídos y el nombre del distrito correspondiente. Dado que la información fue extraída por separada para cada distrito. A continuación se muestra como se crea un data.frame a partir de la lista DOM_luces_distrito DOM_luces_distrito %&lt;&gt;% bind_rows() Para obtener un mapa con los resultados obtenidos se ejecuta la sintaxis siguiente. Donde se realiza un inner_join entre las variables polígonos (poligonos_distrito) y la información satelital (DOM_luces_distrito). Con el resultado obtenido, es posible emplear la función tm_shape de la librería tmap para definir la variable map_dist que contiene las características necesarias para desarrollar el mapa. Ahora, con la función tm_polygons se especifica la variable, el titulo y la paleta de colores que llevará el mapa. Por último, guarda el resultado obtenido con la función tmap_save. poligonos_distrito &lt;- inner_join(poligonos_distrito, DOM_luces_distrito) map_dist &lt;- tm_shape(poligonos_distrito) map_dist &lt;- map_dist + tm_polygons( &quot;F182013_stable_lights&quot;, title = &quot;Luces nocturnas(media)&quot;, palette = &quot;YlOrRd&quot; ) tmap_save(map_dist, &quot;../map_temp/Luces nocturna distr.jpg&quot;, width = 3000, height = 2000, asp = 0 ) El mapa resultante se debe comparar con la información disponible en Google Earth Engine con el propósito de realizar una validación visual entre el imagen del satélite y la información descargada. "],["descarga-de-información-satélite-a-nivel-municipal.html", "2.4 Descarga de información Satélite a nivel municipal", " 2.4 Descarga de información Satélite a nivel municipal Para descargar información satelital del municipio, se sigue un proceso similar al utilizado para descargar información de los distritos, aunque con algunas variaciones en la sintaxis que dependen del tipo de información satelital que se esté obteniendo. Cabe mencionar que las variaciones en la sintaxis se deben a las particularidades de cada tipo de información satelital y no afectan significativamente el proceso de descarga de la misma. Cabe mencionar que, para cada una de las variables satelitales se emplea como medida de resumen la media y la suma. 2.4.1 Luces nocturnas Siguiendo los pasos realizados para el distrito, se hace la lectura del shapefile para los municipio. poligonos_minucipio &lt;- read_sf( &quot;../shapefiles2010/MUNCenso2010.shp&quot;) 2.4.1.1 Medida de resumen la media Note que, el cambio en el código es la variable poligonos_distrito que es sustituida por poligonos_minucipio, de esta forma obtenemos las medidas de resumen por municipios. Además, se incluye una linea de código para guardar los resultados en un archivo de formato .rds. DOM_luces &lt;- map(unique(poligonos_minucipio$ENLACE), ~tryCatch(ee_extract( x = luces, y = poligonos_minucipio[&quot;ENLACE&quot;] %&gt;% filter(ENLACE == .x), ee$Reducer$mean(), sf = FALSE ) %&gt;% mutate(ENLACE = .x), error = function(e)data.frame(ENLACE = .x))) DOM_luces %&lt;&gt;% bind_rows() %&gt;% saveRDS(&quot;../map_temp/data/temp_luces_mean.rds&quot;) 2.4.1.2 Medida de resumen suma En el proceso de la suma la variación del código se presenta en la función de resumen, dado que es sustituido ee$Reducer$mean() por ee$Reducer$sum(), de esta forma se obtiene la suma de los valores de los pixeles dentro del polígono del municipio. DOM_luces &lt;- map(unique(poligonos_minucipio$ENLACE), ~tryCatch(ee_extract( x = luces, y = poligonos_minucipio[&quot;ENLACE&quot;] %&gt;% filter(ENLACE == .x), ee$Reducer$sum(), sf = FALSE ) %&gt;% mutate(ENLACE = .x), error = function(e)data.frame(ENLACE = .x))) DOM_luces %&lt;&gt;% bind_rows() %&gt;% saveRDS(&quot;../map_temp/data/temp_luces_sum.rds&quot;) "],["cubrimiento-de-suelo-urbano-y-cubrimiento-de-suelo-cultivos.html", "2.5 Cubrimiento de suelo urbano y cubrimiento de suelo cultivos", " 2.5 Cubrimiento de suelo urbano y cubrimiento de suelo cultivos La siguiente imagen que se procesa corresponde al cubrimiento urbano y cubrimiento cultivos, para acceder a la información satelital se procede con la función ee$ImageCollection(), y se filtra por fecha con la función ee$ImageCollection$filterDate(). La información satelital se procesa con la función ee$ImageCollection$map(), que permite aplicar una función de resumen a cada imagen satelital presente en la colección. En este caso, se utiliza la función select() para seleccionar únicamente las bandas correspondientes cubrimiento de suelo urbano y al cubrimiento de suelos cultivos. Finalmente, se fusionan las imágenes satelitales resultantes en un único raster con la función ee$ImageCollection$toBands(). tiposuelo = ee$ImageCollection(&quot;COPERNICUS/Landcover/100m/Proba-V-C3/Global&quot;) %&gt;% ee$ImageCollection$filterDate(&quot;2016-01-01&quot;, &quot;2016-12-31&quot;) %&gt;% ee$ImageCollection$map(function(x) x$select(&quot;urban-coverfraction&quot;, &quot;crops-coverfraction&quot;)) %&gt;% ee$ImageCollection$toBands() 2.5.1 Medida de resumen la media Dado que se cambio la fuente de información,es necesario realizar la actualización del código, para este caso cambiamos la variable luces por la variable tiposuelo y conservamos como medida de resumen la media. DOM_suelo &lt;- map(unique(poligonos_minucipio$ENLACE), ~tryCatch(ee_extract( x = tiposuelo, y = poligonos_minucipio[&quot;ENLACE&quot;] %&gt;% filter(ENLACE == .x), ee$Reducer$mean(), sf = FALSE ) %&gt;% mutate(ENLACE = .x), error = function(e)data.frame(ENLACE = .x))) DOM_suelo %&gt;% bind_rows() %&gt;% saveRDS(&quot;../map_temp/data/temp_suelo_mean.rds&quot;) 2.5.2 Medida de resumen suma Continuando el procesamiento de la variable tiposuelo, ahora se realiza la descarga empleando como medida de resumen la suma. DOM_suelo &lt;- map(unique(poligonos_minucipio$ENLACE), ~tryCatch(ee_extract( x = tiposuelo, y = poligonos_minucipio[&quot;ENLACE&quot;] %&gt;% filter(ENLACE == .x), ee$Reducer$sum(), sf = FALSE ) %&gt;% mutate(ENLACE = .x), error = function(e)data.frame(ENLACE = .x))) DOM_suelo %&gt;% bind_rows() %&gt;% saveRDS(&quot;../map_temp/data/temp_suelo_suma.rds&quot;) el procesamiento anterior regresa un data.frame con tres columnas, el ENLACE, que corresponde al código del municipio, X2016_crops.coverfraction y X2016_urban.coverfraction. "],["distancia-a-hospitales.html", "2.6 Distancia a hospitales", " 2.6 Distancia a hospitales Este conjunto de datos se basa en modelos de accesibilidad que se construyen a partir de información sobre la infraestructura de la atención médica y la geografía del terreno. Los modelos se basan en el tiempo de viaje esperado entre diferentes áreas geográficas y los diferentes tipos de instalaciones médicas, como hospitales, clínicas y consultorios médicos. dist_salud = ee$Image(&#39;Oxford/MAP/accessibility_to_healthcare_2019&#39;) 2.6.1 Medida de resumen la media Ahora, la medida de resumen es la media y el proceso es igual a los descritos previamente, la variación de los códigos en la fuente de información. DOM_dist_salud &lt;- map(unique(poligonos_minucipio$ENLACE), ~tryCatch(ee_extract( x = dist_salud, y = poligonos_minucipio[&quot;ENLACE&quot;] %&gt;% filter(ENLACE == .x), ee$Reducer$mean(), sf = FALSE ) %&gt;% mutate(ENLACE = .x), error = function(e)data.frame(ENLACE = .x))) DOM_dist_salud %&gt;% bind_rows() %&gt;% saveRDS(&quot;../map_temp/data/temp_salud_mean.rds&quot;) 2.6.2 Medida de resumen suma DOM_dist_salud &lt;- map(unique(poligonos_minucipio$ENLACE), ~tryCatch(ee_extract( x = dist_salud, y = poligonos_minucipio[&quot;ENLACE&quot;] %&gt;% filter(ENLACE == .x), ee$Reducer$sum(), sf = FALSE ) %&gt;% mutate(ENLACE = .x), error = function(e)data.frame(ENLACE = .x))) DOM_dist_salud %&gt;% bind_rows() %&gt;% saveRDS(&quot;../map_temp/data/temp_salud_suma.rds&quot;) "],["csp-ghm-global-human-modification.html", "2.7 CSP gHM: Global Human Modification", " 2.7 CSP gHM: Global Human Modification Este conjunto de datos muestra la escala y el impacto de la influencia humana en la superficie terrestre, a través de la recopilación y análisis de varios indicadores relacionados con la actividad humana, como la infraestructura construida, el uso del suelo, la densidad de población y la urbanización. La escala del conjunto de datos es de 1 kilómetro cuadrado y se extiende a nivel mundial, proporcionando una visión general de la modificación humana en diferentes áreas del mundo. El conjunto de datos se actualiza regularmente para reflejar los cambios en la modificación humana en el tiempo. CSP_gHM = ee$ImageCollection(&#39;CSP/HM/GlobalHumanModification&#39;) 2.7.1 Medida de resumen la media Como es habitual se hace la modificación del código para la variable CSP_gHM. DOM_CSP_gHM &lt;- map(unique(poligonos_minucipio$ENLACE), ~tryCatch(ee_extract( x = CSP_gHM, y = poligonos_minucipio[&quot;ENLACE&quot;] %&gt;% filter(ENLACE == .x), ee$Reducer$mean(), sf = FALSE ) %&gt;% mutate(ENLACE = .x), error = function(e)data.frame(id_municipio = .x))) DOM_CSP_gHM %&gt;% bind_rows() %&gt;% saveRDS(&quot;../map_temp/data/temp_gHM_mean.rds&quot;) 2.7.2 Medida de resumen suma DOM_CSP_gHM &lt;- map(unique(poligonos_minucipio$ENLACE), ~tryCatch(ee_extract( x = CSP_gHM, y = poligonos_minucipio[&quot;ENLACE&quot;] %&gt;% filter(ENLACE == .x), ee$Reducer$sum(), sf = FALSE ) %&gt;% mutate(ENLACE = .x), error = function(e)data.frame(id_municipio = .x))) DOM_CSP_gHM %&gt;% bind_rows() %&gt;% saveRDS(&quot;../map_temp/data/temp_gHM_suma.rds&quot;) "],["consolidando-las-variables-satelitales.html", "2.8 Consolidando las variables satelitales", " 2.8 Consolidando las variables satelitales Una vez que se han descargado y almacenado todas las variables satelitales en archivos ‘.rds’, es necesario consolidarlos en un único archivo para poder realizar el análisis. Para ello, se utiliza el código siguiente que combina las variables satelitales en un data.frame, para la media y la suma por separado. Además, el código también renombra las variables para hacerlas más descriptivas y fáciles de entender. En este caso se renombran variables como “luces_nocturnas”, cubrimiento_cultivo, cubrimiento_urbano, accesibilidad_hospitales, accesibilidad_hosp_caminado y modificacion_humana para reflejar de manera más clara los datos que representan. Una vez completada la consolidación, se puede comenzar a analizar los datos de manera más detallada. 2.8.1 Medida de resumen la media satelital_promedio &lt;- reduce(list( readRDS(&quot;../map_temp/data/temp_gHM_mean.rds&quot;), readRDS(&quot;../map_temp/data/temp_salud_mean.rds&quot;), readRDS(&quot;../map_temp/data/temp_suelo_mean.rds&quot;), readRDS(&quot;../map_temp/data/temp_luces_mean.rds&quot;)), inner_join) %&gt;% rename( luces_nocturnas = F182013_stable_lights, cubrimiento_cultivo = X2016_crops.coverfraction, cubrimiento_urbano = X2016_urban.coverfraction, accesibilidad_hospitales = accessibility, accesibilidad_hosp_caminado = accessibility_walking_only, modificacion_humana = X2016_gHM) tba(head(satelital_promedio, 15)) ENLACE modificacion_humana accesibilidad_hospitales accesibilidad_hosp_caminado cubrimiento_cultivo cubrimiento_urbano luces_nocturnas 100101 0.8512 0.6314 8.2032 0.4969 82.3803 61.6816 050201 0.3896 52.3864 168.3625 8.0452 3.9507 5.4292 050202 0.3493 38.2478 162.7940 6.1779 1.6302 2.4353 050203 0.2800 56.4654 136.7688 4.2257 0.5977 0.9202 050204 0.2309 73.5606 189.4570 4.5242 0.5840 0.4475 050205 0.3025 105.4853 260.1237 4.0241 1.1075 0.5929 050206 0.4615 33.5826 133.0851 14.9559 3.4069 2.6681 050207 0.6151 13.9386 105.5657 12.0439 4.0286 7.1367 050208 0.3218 48.4565 134.1144 5.3874 1.4284 1.8673 050209 0.2043 134.2056 324.6904 2.6433 0.3586 0.0286 050210 0.2702 56.6110 173.5820 4.6026 0.5170 1.6565 060301 0.4137 56.5053 254.6072 9.3719 2.8411 4.0678 060302 0.3323 46.2911 247.5742 4.5119 1.1275 1.7294 060303 0.3136 58.1096 259.7428 8.3016 1.2898 2.1443 060304 0.3264 84.6066 396.0371 2.4981 2.1171 1.5881 2.8.2 Medida de resumen suma satelital_suma &lt;- reduce(list( readRDS(&quot;../map_temp/data/temp_gHM_suma.rds&quot;), readRDS(&quot;../map_temp/data/temp_salud_suma.rds&quot;), readRDS(&quot;../map_temp/data/temp_suelo_suma.rds&quot;), readRDS(&quot;../map_temp/data/temp_luces_sum.rds&quot;)), inner_join) %&gt;% rename( luces_nocturnas = F182013_stable_lights, cubrimiento_cultivo = X2016_crops.coverfraction, cubrimiento_urbano = X2016_urban.coverfraction, accesibilidad_hospitales = accessibility, accesibilidad_hosp_caminado = accessibility_walking_only, modificacion_humana = X2016_gHM) tba(head(satelital_suma, 15)) ENLACE modificacion_humana accesibilidad_hospitales accesibilidad_hosp_caminado cubrimiento_cultivo cubrimiento_urbano luces_nocturnas 100101 82.5124 57.4419 746.2627 50.6941 8405.0510 5983.3569 050201 171.3378 22531.8859 72414.3451 3716.1412 1824.8588 2393.9961 050202 91.2574 9832.0875 41848.2706 1694.9412 447.2667 636.1412 050203 127.9557 25805.5575 62505.4706 2037.2706 288.1608 420.5569 050204 140.4386 44815.7165 115423.8745 2912.0353 375.8863 272.5804 050205 41.4888 14465.5526 35671.6275 582.4549 160.3020 81.3137 050206 55.5381 4050.7147 16052.6667 1897.8784 432.3294 321.6784 050207 31.3481 703.2686 5326.3059 646.2588 216.1686 363.6078 050208 93.6305 14104.8195 39038.3216 1653.1882 438.3294 543.3216 050209 51.0529 33551.4108 81172.5922 697.5176 94.6353 7.1373 050210 61.0249 12601.1716 38637.9882 1095.7882 123.0863 374.0510 060301 123.7160 16886.6670 76089.6196 2952.5961 895.0941 1215.5412 060302 99.1125 13793.4891 73770.3098 1418.2078 354.4157 515.6078 060303 144.3256 26755.0105 119591.6784 4030.1922 626.1412 987.0039 060304 44.6838 12292.1785 57538.7490 382.9333 324.5333 230.7216 Por último, se guarda el archivo que contiene las dos medidas de resumen en un nuevo ‘.rds’ inner_join( satelital_promedio, satelital_suma, by = &quot;ENLACE&quot;, suffix = c(&quot;_promedio&quot;, &quot;_suma&quot;) ) %&gt;% saveRDS(&quot;../Data/auxiliar_satelital.Rds&quot;) "],["creando-mapas-de-las-variables-satelitales.html", "2.9 Creando mapas de las variables satelitales", " 2.9 Creando mapas de las variables satelitales Durante el proceso de validación del modelo, es fundamental la comparación de los mapas generados con las imágenes disponibles en Google Earth Engine. Para llevar a cabo esta tarea, es necesario realizar la unión entre los polígonos (shapefile) y las data.frame que contienen las variables satelitales. En este sentido, se utiliza el código presentado a continuación para crear los dos objetos: poligonos_promedio y poligonos_suma. poligonos_promedio &lt;- read_sf( &quot;../shapefiles2010/MUNCenso2010.shp&quot;)%&gt;% dplyr::select(ENLACE) %&gt;% inner_join(satelital_promedio) poligonos_suma &lt;- read_sf( &quot;../shapefiles2010/MUNCenso2010.shp&quot;) %&gt;% dplyr::select(ENLACE) %&gt;% inner_join(satelital_suma) El objeto poligonos_promedio se genera a partir del shapefile de los municipios y la información de las variables satelitales que se han promediado. Por otro lado, el objeto poligonos_suma se construye de manera similar, pero utilizando la información de las variables satelitales que se han sumado. El siguiente paso es crear los objeto m1 y m2, para lo cual usamos la función tm_shape de la librería tmap. m1 &lt;- tm_shape(poligonos_suma) m2 &lt;- tm_shape(poligonos_promedio) Para crear los mapas se utiliza una sintaxis similar para cada variable, por lo que se haces pocas modificaciones entre los bloques de código. 2.9.1 Modificacion humana El presente código genera dos mapas utilizando la librería tmap de R. Primero, establece algunas opciones con tmap_options(check.and.fix = TRUE) para verificar y corregir posibles errores. Luego, se crean dos objetos de mapa m1_modificacion_humana y m2_modificacion_humana, que muestran la variable modificacion_humana en dos formas diferentes: la suma y el promedio. Cada uno de los mapas utiliza tm_polygons() para representar los polígonos del mapa y las variables correspondientes. También establece un título para cada mapa con title y una paleta de colores con palette. Finalmente, utiliza tmap_arrange() para combinar los dos mapas en una cuadrícula de dos columnas (ncol = 2) y una fila (nrow = 1), y lo guarda en el objeto map_modificacion_humana. Al final se muestra el mapa combinado. tmap_options(check.and.fix = TRUE) m1_modificacion_humana&lt;- m1 + tm_polygons( &quot;modificacion_humana&quot;, title = &quot;modificacion_humana(suma)&quot;, palette = &quot;YlOrRd&quot; ) m2_modificacion_humana &lt;- m2 + tm_polygons( &quot;modificacion_humana&quot;, title = &quot;modificacion_humana(media)&quot;, palette = &quot;YlOrRd&quot; ) map_modificacion_humana &lt;- tmap_arrange(list(m1_modificacion_humana, m2_modificacion_humana), ncol = 2, norw = 1) El último paso es guardar el mapa resultante, para este fin se utiliza la sintaxis. tmap_save(map_modificacion_humana, &quot;../map_temp/modificacion_humana.jpg&quot;, width = 3000, height = 2000, asp = 0 ) A continuación se presenta un comparativo entre la información obtenida en Google Earth Engine y la procesada en R, donde se puede apreciar que el uso del promedio de los pixeles es una medida de resumen más adecuada, ya que refleja de mejor manera la información capturada por el satélite. Es importante destacar que esta comparación es esencial para validar el proceso de procesamiento de las variables satelitales, ya que permite verificar que la información obtenida en R se asemeje a la imagen de referencia disponible en Google Earth Engine. De esta manera, se asegura que la información procesada sea lo más cercana posible a la realidad y se evita la toma de decisiones basadas en información errónea. 2.9.2 Accesibilidad hospitales caminado Para obtener el mapa correspondiente a la variable accesibilidad_hosp_caminado, se sigue un proceso similar al descrito anteriormente. En primer lugar, se crean los objetos m1_accesibilidad_hosp_caminado y m2_accesibilidad_hosp_caminado utilizando la función tm_shape y tm_polygons de la librería tmap, respectivamente. Estos objetos se unen en el objeto map_accesibilidad_hosp_caminado utilizando la función tmap_arrange para mostrar ambas visualizaciones en una misma imagen. Finalmente, se utiliza la función tmap_save para guardar la imagen resultante en formato PNG. De esta manera, se obtiene un mapa que muestra la accesibilidad a hospitales caminando. m1_accesibilidad_hosp_caminado&lt;- m1 + tm_polygons( &quot;accesibilidad_hosp_caminado&quot;, title = &quot;accesibilidad_hosp_caminado(suma)&quot;, palette = &quot;YlOrRd&quot; ) m2_accesibilidad_hosp_caminado &lt;- m2 + tm_polygons( &quot;accesibilidad_hosp_caminado&quot;, title = &quot;accesibilidad_hosp_caminado(media)&quot;, palette = &quot;YlOrRd&quot; ) map_accesibilidad_hosp_caminado &lt;- tmap_arrange( list( m1_accesibilidad_hosp_caminado, m2_accesibilidad_hosp_caminado ), ncol = 2, norw = 1 ) tmap_save(map_accesibilidad_hosp_caminado, &quot;../map_temp/accesibilidad_hosp_caminado.jpg&quot;, width = 3000, height = 2000, asp = 0 ) La imagen satelital de la variable accesibilidad_hosp_caminado proporciona información sobre los tiempos promedio que una persona tarda en llegar a un hospital o clínica. En la escala de grises utilizada para la gráfica, los tonos más oscuros indican tiempos menores, mientras que los tonos más claros indican tiempos mayores. Al observar la imagen, podemos confirmar que el promedio sigue siendo la medida de resumen que mejor refleja la información capturada por el satélite. 2.9.3 Accesibilidad hospitales La variable accesibilidad_hospitales mide el tiempo promedio que una persona tarda en llegar a clínicas u hospitales, independientemente del medio de transporte utilizado. Como esta variable utiliza la misma fuente de datos que accesibilidad_hosp_caminado, su interpretación visual en escala de grises es similar, donde los tonos más oscuros indican tiempos menores y los tonos más claros, tiempos mayores. Después de observar la imagen, se puede confirmar que el promedio sigue siendo la medida de resumen que mejor refleja la información capturada por el satélite. Para obtener el mapa correspondiente a accesibilidad_hospitales, se sigue un proceso similar al descrito previamente: se crean los objetos m1_accesibilidad_hospitales y m2_accesibilidad_hospitales utilizando tm_shape y tm_polygons de la librería tmap, respectivamente. Luego, se unen en el objeto map_accesibilidad_hospitales utilizando tmap_arrange para mostrar ambas visualizaciones en una misma imagen. Finalmente, se utiliza tmap_save para guardar la imagen resultante en formato PNG. De esta manera, se obtiene un mapa que muestra la accesibilidad a hospitales en la zona de estudio. m1_accesibilidad_hospitales &lt;- m1 + tm_polygons( &quot;accesibilidad_hospitales&quot;, title = &quot;accesibilidad_hospitales(suma)&quot;, palette = &quot;YlOrRd&quot; ) m2_accesibilidad_hospitales &lt;- m2 + tm_polygons( &quot;accesibilidad_hospitales&quot;, title = &quot;accesibilidad_hospitales(media)&quot;, palette = &quot;YlOrRd&quot; ) map_accesibilidad_hospitales &lt;- tmap_arrange( list(m1_accesibilidad_hospitales, m2_accesibilidad_hospitales), ncol = 2, norw = 1 ) tmap_save(map_accesibilidad_hospitales, &quot;../map_temp/accesibilidad_hospitales.jpg&quot;, width = 3000, height = 2000, asp = 0 ) 2.9.4 Cubrimiento urbano La imagen satelital representa el cubrimiento urbano, en una escala de tonos que va desde el negro, que indica ausencia de construcciones urbanas, hasta el blanco, que representa la presencia de edificaciones. Esta variable es relevante porque permite identificar las zonas con mayor densidad de población. Sin embargo, al procesar los datos, se observó que la variable tiene un comportamiento asimétrico y la escala utilizada para representarla no es adecuada para mostrar los detalles más finos de la imagen. Para superar este problema y validar la información obtenida a través de la descarga de datos, se decidió aplicar una transformación logarítmica a la variable. En el siguiente bloque de código se describe el proceso detalladamente para obtener el mapa correspondiente. m1 &lt;- tm_shape(poligonos_suma %&gt;% mutate(cubrimiento_urbano = log(cubrimiento_urbano + 1))) m2 &lt;- tm_shape(poligonos_promedio %&gt;% mutate(cubrimiento_urbano = log(cubrimiento_urbano + 1))) m1_cubrimiento_urbano &lt;- m1 + tm_polygons( &quot;cubrimiento_urbano&quot;, title = &quot;cubrimiento_urbano(suma)&quot;, palette = &quot;YlOrRd&quot; ) m2_cubrimiento_urbano &lt;- m2 + tm_polygons( &quot;cubrimiento_urbano&quot;, title = &quot;cubrimiento_urbano(media)&quot;, palette = &quot;YlOrRd&quot; ) map_cubrimiento_urbano &lt;- tmap_arrange( list(m1_cubrimiento_urbano, m2_cubrimiento_urbano), ncol = 2, norw = 1 ) tmap_save(map_cubrimiento_urbano, &quot;../map_temp/cubrimiento_urbano.jpg&quot;, width = 3000, height = 2000, asp = 0 ) Como se puede notar la modificación realizada consistió en definir nuevamente los objetos m1 y m2, donde incluimos la transformación logarítmica mediante el comando mutate(cubrimiento_urbano = log(cubrimiento_urbano + 1)). Es de mencionar, que para la variable original se le suma la constante uno para corregir errores del calculo, en caso que la variable posea un valor de cero. Ahora, al realizar la comparación entre las dos imágenes podemos vemos que la media, como medida de resumen muestra una mayor similitud con la mostrada del satélite. Como se puede notar, la modificación realizada en el código, en la cual se incluyó la transformación logarítmica de la variable cubrimiento_urbano para obtener un mapa que muestre una mejor correspondencia con la imagen satelital original. En primer lugar, se crearon nuevamente los objetos m1 y m2 utilizando el comando tm_shape. A continuación, se aplicó la transformación logarítmica mediante el comando mutate(cubrimiento_urbano = log(cubrimiento_urbano + 1)), donde se suma una constante de uno para evitar errores en el cálculo en caso de que la variable tenga un valor de cero. Al comparar la imagen obtenida mediante la transformación logarítmica con la imagen satelital original, no es claro cual de las dos imágenes representa de mejor forma la información capturada por el satélite. Dado que, las dos imágenes muestran valores altos en las ciudades principales, sin embargo, la figura de la suma muestra unos municipios en naranja que nos son reflejados por la gráfica obtenida con el promedio. 2.9.5 Cubrimiento cultivo La variable cubrimiento_cultivo proporciona información sobre las áreas de cultivo presentes en los municipios y su interpretación visual en la imagen satelital es similar a las variables analizadas anteriormente. Los tonos más oscuros indican baja presencia de cultivos, mientras que los tonos más claros indican mayor presencia de cultivos. Para poder comparar la información de esta variable con la mostrada por Google Earth Engine, fue necesario aplicar una transformación logarítmica, similar a la realizada en la variable cubrimiento_urbano. El objetivo de esta transformación fue facilitar la comparación entre ambas fuentes de información. Al comparar las dos imágenes, no se observa una clara diferencia en cuanto a cuál medida de resumen sería más adecuada, pero se opta por utilizar el promedio como medida de resumen para esta variable. m1 &lt;- tm_shape(poligonos_suma %&gt;% mutate(cubrimiento_cultivo = log(cubrimiento_cultivo + 1))) m2 &lt;- tm_shape(poligonos_promedio %&gt;% mutate(cubrimiento_cultivo = log(cubrimiento_cultivo + 1))) m1_cubrimiento_cultivo &lt;- m1 + tm_polygons( &quot;cubrimiento_cultivo&quot;, title = &quot;cubrimiento_cultivo(suma)&quot;, palette = &quot;YlOrRd&quot; ) m2_cubrimiento_cultivo &lt;- m2 + tm_polygons( &quot;cubrimiento_cultivo&quot;, title = &quot;cubrimiento_cultivo(media)&quot;, palette = &quot;YlOrRd&quot; ) map_cubrimiento_cultivo &lt;- tmap_arrange( list(m1_cubrimiento_cultivo, m2_cubrimiento_cultivo), ncol = 2, norw = 1 ) tmap_save(map_cubrimiento_cultivo, &quot;../map_temp/cubrimiento_cultivo.jpg&quot;, width = 3000, height = 2000, asp = 0 ) 2.9.6 Luces nocturnas La última variable que validaremos es la intensidad lumínica o luces nocturnas. La variable fue capturada por el satélite en una resolución de 30 metros por píxel. Los valores de la imagen van desde 0 (sin luces) hasta 63 (máxima cantidad de luces), y se pueden utilizar para analizar patrones de urbanización, así como para estimar el crecimiento de ciudades y la densidad de población. Por tanto, se espera que esta variable sea un reflejo de las densidades posesionales de los municipios. Al comparar las imágenes vemos que describen y patrón similar dentro del mapa, sin embargo, la suma muestra un alto valor para el municipio de Higüey dado el tamaño del mismo. Por tanto, se opta por el promedio como medida de resumen para la variable luces_nocturnas. m1_luces &lt;- m1 + tm_polygons( &quot;luces_nocturnas&quot;, title = &quot;luces_nocturnas(suma)&quot;, palette = &quot;YlOrRd&quot; ) m2_luces &lt;- m2 + tm_polygons( &quot;luces_nocturnas&quot;, title = &quot;luces_nocturnas(media)&quot;, palette = &quot;YlOrRd&quot; ) map_luces &lt;- tmap_arrange(list(m1_luces,m2_luces), ncol = 2, norw = 1) tmap_save(map_luces, &quot;../map_temp/luces.jpg&quot;, width = 6920, height = 4080, asp = 0 ) "],["correlación-estimación-directa.html", "2.10 Correlación estimación directa", " 2.10 Correlación estimación directa Dado que el objetivo es tener información variables auxiliares que estén asociada con la estimación directa del indicador y que posean una gran capacidad de predicción realizamos un análisis de correlación entre las variables satelitales (sumas y medias) y las estimaciones directas que fueron calculadas con anterioridad. Para realizar el calculo de las correlaciones fue necesario definir el identificador el dominio (municipio) como se muestra en el siguiente código. satelital_suma %&lt;&gt;% mutate( id_dominio = substring(ENLACE, 3), id_dominio = str_pad( string = id_dominio, width = 4, pad = &quot;0&quot; ) ) satelital_promedio %&lt;&gt;% mutate( id_dominio = substring(ENLACE, 3), id_dominio = str_pad( string = id_dominio, width = 4, pad = &quot;0&quot; ) ) Este código utiliza la función %&lt;&gt;% de la librería magrittr para asignar el resultado de una operación a un objeto sin necesidad de utilizar el operador de asignación &lt;-. El primer bloque de código se aplica al data.frame de satelital_suma y el segundo bloque se aplica satelital_promedio. Ambos bloques de código utilizan la función mutate() de la librería dplyr para crear una nueva columna en cada tabla llamada id_dominio. Esta columna se genera extrayendo una subcadena de la columna ENLACE de cada tabla utilizando la función substring(). El segundo argumento de substring() indica la posición de inicio de la subcadena (en este caso, la tercera posición), mientras que la función str_pad() se utiliza para agregar ceros a la izquierda hasta que la subcadena tenga una longitud de 4 caracteres. Estos bloques de código permiten generar una nueva columna en cada tabla que se utilizará para unir la información satelital con otra información utilizando el código de cada municipio o dominio como clave de unión. "],["estimaciones-directas.html", "2.11 Estimaciones directas", " 2.11 Estimaciones directas En un proceso explicado más adelante en este documento, se realizaron cálculos de estimaciones directas para los dominios en la muestra. Sin embargo, debido a los criterios de calidad a los que se sometieron estas estimaciones directas, no todos los municipios en la muestra pueden ser utilizados en el modelo de área. Las estimaciones que cumplen con los criterios de calidad se guardaron en el archivo base_FH.rds. A continuación, se realiza la lectura de este archivo mediante el siguiente bloque de código. Este código utiliza las tablas estimacion_dir (que se lee desde un archivo RDS) y las tablas satelital_suma y satelital_promedio (que se generan previamente en el código) para realizar un inner_join basado en la columna id_dominio. En el primer bloque de código, se utiliza la función filter() para eliminar las filas que no tienen la varianza estimada (hat_var), y la función dplyr::select() para seleccionar solo las columnas id_dominio y Rd de la tabla. En el segundo bloque de código, se utilizan las funciones inner_join() y select() de la librería dplyr para unir las tablas estimacion_dir y satelital_suma o satelital_promedio, respectivamente, basado en la columna id_dominio. El resultado de esta operación es una nueva tabla que contiene las columnas de ambas tablas unidas por la columna id_dominio. estimacion_dir &lt;- readRDS(&quot;../Data/base_FH.Rds&quot;) %&gt;% filter(!is.na(hat_var)) %&gt;% dplyr::select(id_dominio, Rd) estimacion_dir_suma &lt;- inner_join(estimacion_dir, satelital_suma) estimacion_dir_promedio &lt;- inner_join(estimacion_dir, satelital_promedio) 2.11.1 Correlación de las variables satelital El siguiente código realiza una matriz de correlación entre las variables satelitales y la estimación directa, tanto para la suma como para el promedio. Primero se crea un objeto llamado cor_satelite que se crea a partir de la unión de la correlación entre la estimación directa (Rd) y las demás variables satelitales para la suma y para el promedio. Luego, se asignan los nombres de las filas con los valores Suma y Media. Finalmente, se transpone la matriz y se utiliza la función tba() para presentarla de manera ordenada. cor_satelite &lt;- rbind( cor( estimacion_dir_suma$Rd, estimacion_dir_suma %&gt;% dplyr::select(-ENLACE,-id_dominio, -Rd) ), cor( estimacion_dir_promedio$Rd, estimacion_dir_promedio %&gt;% dplyr::select(-ENLACE,-id_dominio, -Rd) ) ) rownames(cor_satelite) &lt;- c(&quot;Suma&quot;, &quot;Media&quot;) t(cor_satelite) %&gt;% tba(cap = &quot;Corelación de las variables satelitales \\ny la estimación directa&quot;) Tabla 2.1: Corelación de las variables satelitales y la estimación directa Suma Media modificacion_humana -0.2028 -0.5018 accesibilidad_hospitales 0.0140 0.2699 accesibilidad_hosp_caminado -0.0446 0.2151 cubrimiento_cultivo -0.2582 -0.2252 cubrimiento_urbano -0.4682 -0.4248 luces_nocturnas -0.4673 -0.4964 La tabla muestra los coeficientes de correlación entre las variables satelitales (suma y media) y la estimación directa para los dominios en la muestra. Se observa que la modificación humana tiene una correlación negativa con la estimación directa, tanto en la suma como en la media. Por otro lado, la accesibilidad a hospitales muestra una correlación positiva débil con la estimación directa en ambas medidas. La accesibilidad a hospitales caminando y el cubrimiento de cultivos presentan correlaciones negativas en la suma y correlaciones débiles en la media. Finalmente, el cubrimiento urbano y las luces nocturnas presentan correlaciones negativas en ambas medidas. "],["información-auxiliar-en-el-modelo.html", "2.12 Información auxiliar en el modelo", " 2.12 Información auxiliar en el modelo Otra fuente información importante son las variables agregadas obtenidas a partir del Censo, por ellos se hace de mucha importancia realizar el análisis de la variable para identificar de forma visual su aporte en la predicción del modelo de área. Se carga el archivo auxiliar_org.Rds y se eliminaron todas las variables que no son numéricas. A continuación, se seleccionaron solo las variables numéricas para ser utilizadas en el análisis posterior. Además, se agregó una nueva columna llamada id_dominio, que se creó a partir de la columna id_municipio. Para esto, se utilizó la función str_pad para agregar ceros adicionales al inicio de cada valor de la columna id_municipio, de manera que todas las entradas de id_dominio tengan una longitud de 4 dígitos. auxiliar &lt;- readRDS(&quot;../Data/auxiliar_org.Rds&quot;) %&gt;% select_if(is.numeric) %&gt;% mutate( id_dominio = str_pad( string = id_municipio, width = 4, pad = &quot;0&quot; ) ) En el siguiente bloque de código, se procedió a unir la información de los polígonos de los municipios con los datos numéricos contenidos en el archivo auxiliar_org. Para ello, se extrajo el identificador numérico de cada municipio de la variable ENLACE de la tabla poligonos_municipio y se formateó a cuatro dígitos para crear la variable id_dominio. Este identificador se usó para unir los polígonos con los datos numéricos contenidos en auxiliar_org. Se seleccionaron únicamente las variables numéricas contenidas en este último archivo. poligonos_auxiliar &lt;- poligonos_minucipio %&gt;% mutate( id_dominio = substring(ENLACE, 3), id_dominio = str_pad( string = id_dominio, width = 4, pad = &quot;0&quot; ) ) %&gt;% dplyr::select(id_dominio, TOPONIMIA) %&gt;% inner_join(auxiliar) Por último, se crea un mapa para cada variable de la base de datos auxiliar, que contiene información sobre los municipios. Se crea un objeto map_auxiliar a partir de los polígonos de los municipios. Se recorren las columnas de la base de datos auxiliar (excepto la primera, que es la identificación del municipio) y se agrega una capa para cada variable en censal disponible, con un título igual al nombre de la variable. Además, se define una lista path_censo para guardar la ruta donde se guardarán los archivos de imagen de los mapas. map_auxiliar &lt;- tm_shape(poligonos_auxiliar) path_censo &lt;- list() for (ii in names(auxiliar)[-1]) { m1_auxiliar &lt;- map_auxiliar + tm_polygons(ii, title = ii, palette = &quot;YlOrRd&quot;) path_censo[[ii]] &lt;- paste0(&quot;../map_temp/&quot;, ii, &quot;.jpg&quot;) # tmap_save( # m1_auxiliar, # paste0(&quot;../map_temp/&quot;, ii, &quot;.jpg&quot;), # width = 3000, # height = 2000, # asp = 0 # ) } ## Densidad_Pob P45_TUVO_EMPLEO ## &quot;../map_temp/Densidad_Pob.jpg&quot; &quot;../map_temp/P45_TUVO_EMPLEO.jpg&quot; ## P46_ACTIVIDAD_PORPAGA P47_AYUDO_SINPAGA ## &quot;../map_temp/P46_ACTIVIDAD_PORPAGA.jpg&quot; &quot;../map_temp/P47_AYUDO_SINPAGA.jpg&quot; ## P30_DONDE_NACE ## &quot;../map_temp/P30_DONDE_NACE.jpg&quot; "],["estandarizar-insumos.html", "Capítulo 3 Estandarizar insumos", " Capítulo 3 Estandarizar insumos El modelo Fay-Herriot utiliza información auxiliar, en forma de covariables, para mejorar la precisión de la estimación. Este conjunto de sintaxis en R tiene como objetivo preparar los datos necesarios para la aplicación del modelo Fay-Herriot para la estimación directa, utilizando la transformación arcoseno y la Función Generalizada de Varianza. Para lograr esto, se utilizarán diferentes fuentes de información (encuestas, registros administrativos, imágenes satelitales, shapefile y el Censo). Por esta razón, es fundamental la estandarización de las bases de datos, que consiste en tener una uniformidad de la estructura, formato y contenido de los datos, lo que permite su comparación y combinación sin errores o confusiones. La estandarización de bases de datos es esencial para la integración de datos provenientes de diferentes fuentes, lo que facilita la toma de decisiones durante el proceso y la obtención de resultados precisos y confiables. En resumen, la estandarización de bases de datos es crucial para garantizar la calidad y confiabilidad de la información utilizada en los procesos de análisis. "],["lectura-de-librerias.html", "3.1 Lectura de librerias", " 3.1 Lectura de librerias El código presenta una serie de librerías de R que se utilizan para la preparación de los datos. La librería tidyverse proporciona un conjunto de paquetes para la procesamiento y visualización de datos. magrittr se utiliza para facilitar la lectura del código, mientras que stringr es una librería que proporciona herramientas para la manipulación de cadenas de caracteres. readxl es una librería utilizada para leer archivos de Excel. tmap y sp se utilizan para la visualización de mapas, mientras que sf es una librería que proporciona herramientas para la manipulación de datos espaciales. En conjunto, estas librerías permiten la estandarización de bases de datos y la posterior integración de información proveniente de diferentes fuentes. library(tidyverse) library(magrittr) library(stringr) library(readxl) library(tmap) library(sp) library(sf) "],["lectura-de-encuesta..html", "3.2 Lectura de encuesta.", " 3.2 Lectura de encuesta. El código cargará un archivo encuestaDOM.Rds que contiene los datos de la encuesta realizada. Luego, utiliza la función mutate() del paquete dplyr para crear dos nuevas columnas id_dominio e id_region que representan el identificador del municipio y la región respectivamente. La función str_pad() del paquete stringr se utiliza para agregar ceros a la izquierda de los números, lo que asegura que los identificadores tengan el mismo número de dígitos. El operador %&lt;&gt;% se utiliza para encadenar las funciones y guardar el resultado en la misma variable encuestaDOM. encuestaDOM &lt;- readRDS(&quot;../Data/encuestaDOM.Rds&quot;) encuestaDOM %&lt;&gt;% mutate(id_dominio = str_pad(string = id_municipio, width = 4, pad = &quot;0&quot;), id_region = str_pad(string = orden_region, width = 2, pad = &quot;0&quot;)) "],["lectura-de-información-auxiliar-covariable.html", "3.3 Lectura de información auxiliar (covariable)", " 3.3 Lectura de información auxiliar (covariable) Este código carga el archivo auxiliar_org.Rds que contiene la información Censal y luego agrega una columna llamada id_dominio, la cual consiste en el valor de la columna id_municipio rellenada con ceros a la izquierda para completar 4 dígitos. La función utilizada para rellenar con ceros es str_pad de la librería stringr. auxiliar_org &lt;- readRDS(&quot;../Data/auxiliar_org.Rds&quot;) %&gt;% mutate(id_dominio = str_pad( string = id_municipio, width = 4, pad = &quot;0&quot; )) El código tiene como objetivo crear una base de datos auxiliar a partir de la información satelital y otra de la información del Censo. Primero se carga la información del archivo auxiliar_satelital y se estandarizan las variables numéricas mediante la función scale() Luego se realiza una manipulación de la variable ENLACE y se crea una nueva variable id_dominio mediante la función str_pad(). Después se realiza una unión interna de las bases de datos auxiliar_org y auxiliar_satelital por la variable id_dominio, para crear la base de datos statelevel_predictors, la cual servirá de información auxilia auxiliar_satelital &lt;- readRDS(&quot;../Data/auxiliar_satelital.Rds&quot;) %&gt;% mutate_if(is.numeric, function(x)as.numeric(scale(x))) %&gt;% mutate( ENLACE = substring(ENLACE, 3), id_dominio = str_pad( string = ENLACE, width = 4, pad = &quot;0&quot; ) ) statelevel_predictors &lt;- inner_join(auxiliar_org, auxiliar_satelital, by = &quot;id_dominio&quot;) "],["otra-fuente-de-información.html", "3.4 Otra fuente de información", " 3.4 Otra fuente de información El fragmento de código presenta la carga de un archivo Excel que contiene datos del DEE agrupados por municipios. Los datos son modificados mediante la función mutate() para incluir un identificador de dominio y se elimina la columna Cod. DEE_Mun &lt;- read_xlsx(&#39;../Data/datos del DEE ONE agrupdos por municipios.xlsx&#39;) %&gt;% mutate(id_dominio = str_pad( string = Cod, width = 4, pad = &quot;0&quot; ), Cod =NULL) "],["definiendo-el-identificador-de-dominio-en-la-shapefile..html", "3.5 Definiendo el identificador de dominio en la shapefile.", " 3.5 Definiendo el identificador de dominio en la shapefile. Este código carga un archivo de shapefile y lo convierte en un objeto de la clase sf utilizando la función read_sf() de la librería sf. Posteriormente, se realiza una mutación en la cual se crea una nueva variable llamada id_dominio que corresponde a los cuatro últimos dígitos de la variable ENLACE que se encuentra en el shapefile. Esto se logra mediante la función substring() y la función str_pad() de la librería stringr. Finalmente, se selecciona la variable id_dominio y geometry del objeto sf utilizando la función select. Shapefile &lt;- read_sf( &quot;../shapefiles2010/MUNCenso2010.shp&quot; ) %&gt;% mutate( ENLACE = substring(ENLACE,3), id_dominio = str_pad( string = ENLACE, width = 4, pad = &quot;0&quot; ) ) %&gt;% select(id_dominio, geometry) "],["validando-id_dominio.html", "3.6 Validando id_dominio", " 3.6 Validando id_dominio La validación del identificador id_dominio es importante para asegurarnos de que todas las bases de datos que se utilizarán en el modelo tienen una correspondencia adecuada entre ellas. En el código presentado se realizan tres validaciones utilizando la función full_join() y la función distinct(). En la primera validación, se comparan los identificadores de encuestaDOM con los de statelevel_predictors. En la segunda validación, se comparan los identificadores de encuestaDOM con los de DEE_Mun. Finalmente, en la tercera validación, se comparan los identificadores de encuestaDOM con los de Shapefile. En todas las validaciones se utilizó la función head() para mostrar las primeras 10 filas y la función tba() para visualizar los resultados en forma de tabla. De esta forma, se puede verificar que el identificador id_dominio es consistente en todas las bases de datos utilizadas. encuestaDOM %&gt;% distinct(id_dominio, DES_PROVINCIA) %&gt;% full_join(statelevel_predictors %&gt;% distinct(id_dominio)) %&gt;% head(10) %&gt;% tba() id_dominio DES_PROVINCIA 0101 DISTRITO NACIONAL 3201 SANTO DOMINGO 3202 SANTO DOMINGO 3203 SANTO DOMINGO 3204 SANTO DOMINGO 3205 SANTO DOMINGO 3206 SANTO DOMINGO 3207 SANTO DOMINGO 2501 SANTIAGO 2502 SANTIAGO encuestaDOM %&gt;% distinct(id_dominio, DES_PROVINCIA) %&gt;% full_join(DEE_Mun %&gt;% distinct(id_dominio, Des))%&gt;% head(10) %&gt;% tba() id_dominio DES_PROVINCIA Des 0101 DISTRITO NACIONAL Santo Domingo de Guzmán 3201 SANTO DOMINGO Santo Domingo Este 3202 SANTO DOMINGO Santo Domingo Oeste 3203 SANTO DOMINGO Santo Domingo Norte 3204 SANTO DOMINGO Boca Chica 3205 SANTO DOMINGO San Antonio de Guerra 3206 SANTO DOMINGO Los Alcarrizos 3207 SANTO DOMINGO Pedro Brand 2501 SANTIAGO Santiago 2502 SANTIAGO Bisonó encuestaDOM %&gt;% distinct(id_dominio, DES_PROVINCIA) %&gt;% full_join(Shapefile %&gt;% data.frame() %&gt;% select(id_dominio))%&gt;% head(10) %&gt;% tba() id_dominio DES_PROVINCIA 0101 DISTRITO NACIONAL 3201 SANTO DOMINGO 3202 SANTO DOMINGO 3203 SANTO DOMINGO 3204 SANTO DOMINGO 3205 SANTO DOMINGO 3206 SANTO DOMINGO 3207 SANTO DOMINGO 2501 SANTIAGO 2502 SANTIAGO "],["guardar-archivos.html", "3.7 Guardar archivos", " 3.7 Guardar archivos Las instrucciones presentadas permiten guardar las bases de datos que se utilizarán en el modelo Fay-Herriot para la estimación directa. Las funciones saveRDS() y st_write() se utilizan para guardar las bases en formatos compatibles con el software R, permitiendo su posterior uso en otras sesiones de trabajo. Es importante guardar las bases de datos una vez que se han validado los identificadores, para asegurarse de que no hay errores en el proceso de identificación de las observaciones. saveRDS(encuestaDOM, file = &quot;../Data/encuestaDOM.Rds&quot;) saveRDS(statelevel_predictors, file = &quot;../Data/statelevel_predictors_df.rds&quot;) saveRDS(DEE_Mun, file = &quot;../Data/DEE_Mun.Rds&quot;) st_write(obj = Shapefile,&quot;../shapefiles2010/DOM.shp&quot;) "],["agregados-censales-municipios-y-región..html", "3.8 Agregados censales (municipios y región).", " 3.8 Agregados censales (municipios y región). Se importaron las bases de agregados censales por municipios y por región. Se utilizó la función readRDS() para importar la base encuestaDOMRegion.Rds que contiene información del Censo por región y la función transmute() para seleccionar y renombrar las variables de interés. La base resultante agregado_region se guardó en un archivo .rds. agregado_region &lt;- readRDS(&quot;../Data/encuestaDOMRegion.Rds&quot;) %&gt;% transmute(pp_region = hh_depto, id_region = str_pad(string = grupo_region, width = 2, pad = &quot;0&quot;)) tba(agregado_region) pp_region id_region 3339410 01 3246032 02 1692085 03 1167754 04 saveRDS(agregado_region, file = &quot;../Data/agregado_persona_region.rds&quot;) Además, se cargó la base personas_dominio.Rds que contiene la cantidad de personas por dominio y se utilizó la función mutate() para crear la variable id_region y transformar la variable total_pp en pp_dominio. Finalmente, se eliminaron las variables total_pp e id_municipio. La base resultante se guardó en un archivo .rds. readRDS(&quot;../Data/personas_dominio.Rds&quot;) %&gt;% mutate( id_region = str_pad(string = orden_region, width = 2, pad = &quot;0&quot;), orden_region = NULL, pp_dominio = total_pp, total_pp = NULL, id_municipio = NULL) %&gt;% saveRDS(file = &quot;../Data/agregado_persona_dominio.rds&quot;) "],["estimación-de-directa-por-dominios.html", "Capítulo 4 Estimación de directa por dominios", " Capítulo 4 Estimación de directa por dominios En este apartado realizaremos las estimaciones directas para los dominios que fueron seleccionados en la muestra, dado que estos fueron no planeados. Las estimaciones directas son una herramienta comúnmente utilizada en la estadística inferencial para obtener información sobre una población a partir de una muestra. Sin embargo, estas estimaciones pueden presentar problemas cuando la muestra es pequeña, lo que puede conducir a una falta de precisión en las estimaciones y a una mayor incertidumbre en las conclusiones que se puedan extraer. "],["lectura-de-librerías.html", "4.1 Lectura de librerías", " 4.1 Lectura de librerías Las librerías survey, tidyverse, srvyr y TeachingSampling son fundamentales para el análisis de datos de encuestas. survey es una librería que proporciona herramientas para el análisis de datos complejos de encuestas, permitiendo la incorporación de pesos y diseños complejos de muestreo. library(survey) library(tidyverse) library(srvyr) library(TeachingSampling) "],["lectura-de-la-encuesta-y-definición-de-parámetros.html", "4.2 Lectura de la encuesta y definición de parámetros", " 4.2 Lectura de la encuesta y definición de parámetros Este código carga los datos de la encuesta y define la variable id_dominio que se utiliza como identificador único del dominio de estudio a lo largo del código. Luego, se realiza una serie de transformaciones en los datos mediante la función mutate() del paquete dplyr. La primera transformación utiliza la función str_pad() para agregar ceros a la izquierda de la variable upm hasta que tenga una longitud de 9 caracteres. La variable upm es un identificador único para cada Unidad Primaria de Muestreo (UPM) en la encuesta. La segunda transformación hace lo mismo para la variable estrato Por último, se crea una nueva variable llamada factor_anual que representa el factor de expansión de la encuesta dividido por 4. Este factor se utiliza para ajustar las estimaciones de la encuesta y obtener estimaciones representativas de la población objetivo. encuestaDOM &lt;- readRDS(&quot;../Data/encuestaDOM.Rds&quot;) id_dominio &lt;- &quot;id_dominio&quot; encuestaDOM &lt;- encuestaDOM %&gt;% mutate( upm = str_pad(string = upm,width = 9,pad = &quot;0&quot;), estrato = str_pad(string = estrato,width = 5,pad = &quot;0&quot;), factor_anual = factor_expansion / 4 ) "],["definición-del-diseño-muestral-con-la-libreria-survey.html", "4.3 Definición del diseño muestral con la libreria survey", " 4.3 Definición del diseño muestral con la libreria survey Este código establece el diseño muestral complejo para el análisis de la Encuesta. En particular, se establecen las opciones de ajuste para el tratamiento de los PSUs solitarios (unidades primarias de muestreo con un solo elemento). Luego, se define el diseño utilizando la función as_survey_design() del paquete survey, especificando las variables de estratificación, el identificador de las UPM, los pesos de muestreo y la opción nest=T para indicar que el diseño es anidado, es decir, que los datos de cada unidad secundaria de muestreo (hogar) están contenidos dentro de su Unidad Primaria de Muestreo. options(survey.lonely.psu= &#39;adjust&#39; ) disenoDOM &lt;- encuestaDOM %&gt;% as_survey_design( strata = estrato, ids = upm, weights = factor_anual, nest=T ) "],["calculo-del-indicador.html", "4.4 Calculo del indicador", " 4.4 Calculo del indicador Este bloque de código realiza lo siguiente: Se agrupa la encuesta por id_dominio. Se filtran los casos en los que la variable ocupado es igual a 1 y la variable pet es igual a 1. Se calcula el tamaño muestral no ponderado (n()). Se calcula la razón de la variable orden_sector igual a 2 sobre la variable constante igual a 1 mediante el uso de survey_ratio(), que utiliza los pesos de muestreo para producir estimaciones de varianza y errores estándar apropiados para el muestreo complejo. La función survey_ratio() también permite calcular intervalos de confianza y coeficientes de variación. indicador_dom &lt;- disenoDOM %&gt;% group_by_at(id_dominio) %&gt;% filter(ocupado == 1 &amp; pet == 1) %&gt;% summarise( n = unweighted(n()), Rd = survey_ratio( numerator = orden_sector == 2 , denominator = 1, vartype = c(&quot;se&quot;, &quot;ci&quot;, &quot;var&quot;, &quot;cv&quot;), deff = T ) ) Este código realiza un full_join entre indicador_dom y un data.frame que contiene únicamente las variables id_dominio y des_municipio de encuestaDOM. El resultado de esta unión es que a indicador_dom se le agrega la variable des_municipio correspondiente al nombre de cada municipio. indicador_dom &lt;- full_join(indicador_dom, distinct(( encuestaDOM %&gt;% dplyr::select(id_dominio, des_municipio) )), by = id_dominio) 4.4.1 Guardar resultado de la estimación directa saveRDS(indicador_dom,&#39;Data/indicador_dom.Rds&#39; ) "],["función-generalizada-de-varianza.html", "Capítulo 5 Función generalizada de varianza", " Capítulo 5 Función generalizada de varianza Uno de los insumos más importantes en el modelo de áreas es la varianza del estimador directo, a nivel de dominio, la cual no puede calcularse de ningún modo. En correspondencia, este valor debe estimarse desde los datos recolectados en cada dominio. Sin embargo, en dominios en las que se cuenta con un tamaño de muestra muy pequeño, estas estimaciones no tendrán un buen comportamiento. Por ende, es muy útil utilizar un modelo de suavizamiento de las varianzas para eliminar el ruido y la volatilidad de estas estimaciones y extraer la verdadera señal del proceso Hidiroglou (2019) afirma que \\(E_{mp}\\left(\\hat{\\theta}^{dir}_d\\right)=\\boldsymbol{x}_{d}\\boldsymbol{\\beta}\\) y \\(V_{mp}\\left(\\hat{\\theta}^{dir}_d\\right)=\\sigma_{u}^2+\\tilde{\\psi}^2_{d}\\), en donde el subíndice \\(mp\\) hace referencia a la inferencia doble que se debe tener en cuenta en este tipo de ajustes y define la medida de probabilidad conjunta entre el modelo y el diseño de muestreo. \\(m\\) hace referencia a la medida de probabilidad inducida por el modelamiento y la inclusión de las covariables auxiliares (\\(\\boldsymbol{x}_{d}\\)). \\(p\\) hacer referencia a la medida de probabilidad inducida por el diseño de muestreo complejo que induce las estimaciones directas. La solución que acá se plantea se conoce con el nombre de Función Generalizada de Varianza, la cual consiste en ajustar un modelo log-lineal a la varianza directa estimada. Partiendo del hecho de que se tiene acceso a un estimador insesgado de \\(\\psi^2\\), denotado por \\(\\hat{\\psi}^2\\) se tiene que: \\[ E_{mp}\\left(\\hat{\\psi}_{d}^{2}\\right)=E_{m}\\left(E_{p}\\left(\\psi_{d}^{2}\\right)\\right)=E_{m}\\left(\\psi_{d}^{2}\\right)=\\tilde{\\psi}_{d}^{2} \\] La anterior igualdad puede interpretarse como que un estimador insesgado y simple de \\(\\tilde{\\psi}_{d}^{2}\\) puede ser \\(\\hat{\\psi}_{d}^{2}\\). Sin embargo, este estimador de muestreo es inestable cuando el tamaño de muestra es pequeño, que es justo el paradigma dominante en la estimación de áreas pequeñas. Rivest and Belmonte (2000) consideran modelos de suavizamiento para la estimación de las varianzas directas definidos de la siguiente manera: \\[ \\log\\left(\\hat{\\psi}_{d}^{2}\\right)=\\boldsymbol{z}_{d}^{t}\\boldsymbol{\\alpha}+\\boldsymbol{\\varepsilon}_{d} \\] En donde \\(\\boldsymbol{z}_{d}\\) es un vector de covariables explicativas que son funciones de \\(\\boldsymbol{x}_{d}\\), \\(\\boldsymbol{\\alpha}\\) es un vector de parámetros que deben ser estimados, \\(\\boldsymbol{\\varepsilon}_{d}\\) son errores aleatorios con media cero y varianza constante, que se asumen idénticamente distribuidos condicionalmente sobre \\(\\boldsymbol{z}_{d}\\). Del anterior modelo, la estimación suavizada de la varianza de muestreo está dada por: \\[ \\tilde{\\psi}_{d}^{2}=E_{mp}\\left(\\psi_{d}^{2}\\right)=\\exp\\left(\\boldsymbol{z}_{d}^{t}\\boldsymbol{\\alpha}\\right)\\times\\Delta \\] En donde, \\(E_{mp}\\left(\\varepsilon_{d}\\right)=\\Delta\\). No hay necesidad de especificar una distribución paramétrica para los errores de este modelo. Al utilizar el método de los momentos, se tiene el siguiente estimador insesgado para \\(\\Delta\\): \\[ \\hat{\\Delta}=\\frac{\\sum_{d=1}^{D}\\hat{\\psi}_{d}^{2}}{\\sum_{d=1}^{D}\\exp\\left(\\boldsymbol{z}_{d}^{t}\\boldsymbol{\\alpha}\\right)} \\] De la misma forma, al utilizar los procedimientos estándar en una regresión lineal, la estimación del coeficiente de parámetros de regresión está dada por la siguiente expresión: \\[ \\hat{\\boldsymbol{\\alpha}}=\\left(\\sum_{d=1}^{D}\\boldsymbol{z}_{d}\\boldsymbol{z}_{d}^{t}\\right)^{-1}\\sum_{d=1}^{D}\\boldsymbol{z}_{d}\\log\\left(\\hat{\\psi}_{d}^{2}\\right) \\] Por último, el estimador suavizado de la varianza muestral está definido por: \\[ \\hat{\\tilde{\\psi}}_{d}^{2}=\\exp\\left(\\boldsymbol{z}_{d}^{t}\\hat{\\boldsymbol{\\alpha}}\\right)\\hat{\\Delta} \\] "],["lectura-de-librerías-1.html", "5.1 Lectura de librerías", " 5.1 Lectura de librerías En cada etapa del proceso, es común comenzar cargando las librerías necesarias. Para ajustar el modelo log-lineal, se utilizó el paquete base de R. Además, se generaron gráficos para el análisis exploratorio de los datos, utilizando las librerías ggplot2 y patchwork. Para el procesamiento de la base, se implementó el paquete dplyr y se definió la función select() como medida de prevención de conflictos entre funciones de otros paquetes. Además, se estableció la variable id_dominio como parte del proceso de estandarización. library(ggplot2) library(dplyr) library(patchwork) select &lt;- dplyr::select id_dominio &lt;- &quot;id_dominio&quot; "],["lectura-del-la-base-de-datos.html", "5.2 Lectura del la base de datos", " 5.2 Lectura del la base de datos El código presentado es utilizado para procesar una base de datos de una encuesta en formato .Rds. Primero se carga la base de datos y se agrega un cero al inicio de la variable UPM con la funciónstr_pad(). Posteriormente, se crea un objeto n_upm que contiene el número de Unidades Primarias de Muestreo (UPM) por dominio, esto se logra a través de la función distinct() para seleccionar las columnas que cumplen ciertas condiciones y la función group_by() para agrupar los datos por dominio. Finalmente, se utiliza tally() para contar el número de UPM por dominio y se presenta la tabla resultante con la funcióntba(). encuestaDOM &lt;- readRDS(&quot;../Data/encuestaDOM.Rds&quot;) %&gt;% mutate( upm = str_pad(string = upm,width = 9,pad = &quot;0&quot;)) n_upm &lt;- encuestaDOM %&gt;% distinct(id_dominio, upm) %&gt;% group_by(id_dominio) %&gt;% tally(name = &quot;n_upm&quot;,sort = TRUE) tba(head(n_upm,10)) id_dominio n_upm 0101 127 3201 109 2501 87 3203 59 3202 42 1101 38 3206 32 0901 20 1301 20 2101 20 "],["lectura-de-las-estimaciones-directas-del-indicador.html", "5.3 Lectura de las estimaciones directas del indicador", " 5.3 Lectura de las estimaciones directas del indicador El código está realizando una unión completa (full join) de dos bases de datos indicador_dom y n_upm, usando la variable id_dominio como clave. La unión completa asegura que se conserven todas las filas de ambas bases de datos, incluso aquellas que no tienen coincidencias en la otra base de datos. Después de la unión, la variable n_upm (que indica el número de UPMs por dominio) se añade a la base de datos indicador_dom. indicador_dom &lt;- readRDS(&#39;../Data/indicador_dom.Rds&#39;) indicador_dom &lt;- full_join(indicador_dom, n_upm, by = id_dominio) Se filtran los dominios que posean una varianza estimada mayor que cero, un deff mayor que 1 y 2 o más UPMs indicador_dom1 &lt;- indicador_dom %&gt;% filter(Rd_var&gt;0 &amp; Rd_deff&gt;=1 &amp; n_upm &gt;= 2) para los dominios que superan estas condiciones se realiza la transformación \\(\\log(\\hat{\\sigma}^2_d)\\), además se realiza la selección de las columnas identificador del municipio (id_dominio), la estimación directa del indicador (Rd), El número de personas en el dominio (n) y la varianza estimada del para la estimación directa Rd_var,siendo esta la que transforma mediante la función log(). baseFGV &lt;- indicador_dom1 %&gt;% dplyr::select(id_dominio , Rd, n, Rd_var) %&gt;% mutate(ln_sigma2 = log(Rd_var)) "],["gráficas-exploratorias.html", "5.4 Gráficas exploratorias", " 5.4 Gráficas exploratorias El código muestra la creación de cuatro gráficos usando la librería ggplot2 y el uso de los datos baseFGV. Estos gráficos tienen como objetivo explorar la relación entre el logaritmo de la varianza y diferentes transformaciones de la n y Rd. El primer gráfico (p1) representa la relación entre la estimación directa y el logaritmo de la varianza. El segundo gráfico (p2) representa la relación entre el tamaño de muestra y el logaritmo de la varianza. El tercer gráfico (p3) representa la relación entre \\(n_d \\times Rd\\) y el logaritmo de la varianza. Finalmente, el cuarto gráfico (p4) representa la relación entre la raíz cuadrada de la estimación directa y el logaritmo de la varianza. p1 &lt;- ggplot(baseFGV, aes(x = Rd, y = ln_sigma2)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;Formal&quot;) p2 &lt;- ggplot(baseFGV, aes(x = n, y = ln_sigma2)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;Tamaño de muestra&quot;) p3 &lt;- ggplot(baseFGV, aes(x = Rd * n, y = ln_sigma2)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;Número de Formales&quot;) p4 &lt;- ggplot(baseFGV, aes(x = sqrt(Rd), y = ln_sigma2)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;Raiz cuadrada de tasa de formalidad&quot;) (p1 | p2) / (p3 | p4) rm(&#39;p1&#39;,&#39;p2&#39;,&#39;p3&#39;,&#39;p4&#39;) "],["ajustando-el-modelo-log-lineal-de-la-varianza.html", "5.5 Ajustando el modelo log-lineal de la varianza", " 5.5 Ajustando el modelo log-lineal de la varianza El código ajusta un modelo de regresión lineal múltiple (utilizando la función lm()), donde ln_sigma2 es la variable respuesta y las variables predictoras son Rd, n, y varias transformaciones de éstas. El objetivo de este modelo es estimar la función generalizada de varianza (FGV) para los dominios observados. library(gtsummary) FGV1 &lt;- lm(ln_sigma2 ~ 1 + Rd + n + I(n ^ 2) + I(Rd * n) + I(sqrt(Rd)) + I(sqrt(n)) + I(sqrt(Rd * n)) , data = baseFGV) tbl_regression(FGV1) %&gt;% add_glance_table(include = c(r.squared, adj.r.squared)) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #lkbcxihqec .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #lkbcxihqec .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #lkbcxihqec .gt_caption { padding-top: 4px; padding-bottom: 4px; } #lkbcxihqec .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #lkbcxihqec .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #lkbcxihqec .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lkbcxihqec .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #lkbcxihqec .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #lkbcxihqec .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #lkbcxihqec .gt_column_spanner_outer:first-child { padding-left: 0; } #lkbcxihqec .gt_column_spanner_outer:last-child { padding-right: 0; } #lkbcxihqec .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #lkbcxihqec .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #lkbcxihqec .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #lkbcxihqec .gt_from_md > :first-child { margin-top: 0; } #lkbcxihqec .gt_from_md > :last-child { margin-bottom: 0; } #lkbcxihqec .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #lkbcxihqec .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #lkbcxihqec .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #lkbcxihqec .gt_row_group_first td { border-top-width: 2px; } #lkbcxihqec .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #lkbcxihqec .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #lkbcxihqec .gt_first_summary_row.thick { border-top-width: 2px; } #lkbcxihqec .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lkbcxihqec .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #lkbcxihqec .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #lkbcxihqec .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #lkbcxihqec .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lkbcxihqec .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #lkbcxihqec .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #lkbcxihqec .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #lkbcxihqec .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #lkbcxihqec .gt_left { text-align: left; } #lkbcxihqec .gt_center { text-align: center; } #lkbcxihqec .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #lkbcxihqec .gt_font_normal { font-weight: normal; } #lkbcxihqec .gt_font_bold { font-weight: bold; } #lkbcxihqec .gt_font_italic { font-style: italic; } #lkbcxihqec .gt_super { font-size: 65%; } #lkbcxihqec .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #lkbcxihqec .gt_asterisk { font-size: 100%; vertical-align: 0; } #lkbcxihqec .gt_indent_1 { text-indent: 5px; } #lkbcxihqec .gt_indent_2 { text-indent: 10px; } #lkbcxihqec .gt_indent_3 { text-indent: 15px; } #lkbcxihqec .gt_indent_4 { text-indent: 20px; } #lkbcxihqec .gt_indent_5 { text-indent: 25px; } Characteristic Beta 95% CI1 p-value Rd -22 -50, 5.8 0.12 n -0.01 -0.03, 0.01 0.3 I(n^2) 0.00 0.00, 0.00 0.5 I(Rd * n) 0.02 -0.01, 0.05 0.2 I(sqrt(Rd)) 45 -4.7, 94 0.076 I(sqrt(n)) 0.87 -0.48, 2.2 0.2 I(sqrt(Rd * n)) -1.4 -3.1, 0.34 0.11 R² 0.683 Adjusted R² 0.656 1 CI = Confidence Interval Después de tener la estimación del modelo se debe obtener el valor de la constante \\(\\Delta\\) para lo cual se usa el siguiente código. delta.hat = sum(baseFGV$Rd_var) / sum(exp(fitted.values(FGV1))) De donde se obtiene que \\(\\Delta = 1.2364741\\). Final es posible obtener la varianza suavizada ejecutando el siguiente comando. baseFGV &lt;- baseFGV %&gt;% mutate(hat_var = delta.hat * exp(fitted.values(FGV1))) "],["validaciones-sobre-el-modelo.html", "5.6 Validaciones sobre el modelo", " 5.6 Validaciones sobre el modelo par(mfrow = c(2, 2)) plot(FGV1) varianza suavizada Vs varianza estimada ggplot(baseFGV, aes(x = Rd_var, y = hat_var)) + geom_point() + geom_smooth(method = &quot;loess&quot;) Este código está realizando una Consolidación de los dominios observados y no observados para lo cual hace una unión izquierda (left_join()) entre: indicador_dom y baseFGV de la cual selecciona las columnas de id_dominio y hat_var. El argumento by = id_dominio especifica que la unión debe realizarse mediante la columna id_dominio. Luego, se utiliza la función mutate() para crear dos nuevas variables. La primera variable Rd_var se asigna el valor de Rd_var de baseFGV si hat_var no es un valor nulo (NA), de lo contrario se le asigna un valor NA_real_ (NA pero de tipo numérico). De manera similar, se crea la variable Rd_deff con el valor de Rd_deff de baseFGV si hat_var no es nulo, de lo contrario se le asigna un valor NA_real_. base_sae &lt;- left_join(indicador_dom, baseFGV %&gt;% select(id_dominio, hat_var), by = id_dominio) %&gt;% mutate( Rd_var = ifelse(is.na(hat_var), NA_real_, Rd_var), Rd_deff = ifelse(is.na(hat_var), NA_real_, Rd_deff) ) Ahora, se debe estimar deff_FGV y n_eff_FGV a parir de la varianza suvizada (hat_var). base_FH &lt;- base_sae %&gt;% mutate( Rd_deff = ifelse(is.nan(Rd_deff), 1, Rd_deff), deff_FGV = ifelse(Rd_var == 0 , 1, hat_var / (Rd_var / Rd_deff) #Fórmula del nuevo DEFF ), # Criterio MDS para regularizar el DeffFGV deff_FGV = ifelse(deff_FGV &lt;= 1, NA_real_, deff_FGV), #Deff estimado n_eff_FGV = n / deff_FGV, #Número efectivo de personas encuestadas # Si no se estimó varianza para ese municipio, también excluir # la estimación directa de este municipio, esto es relevante para el modelo FH hat_var = ifelse(deff_FGV &lt;= 1, NA_real_, hat_var), Rd = ifelse(is.na(hat_var), NA_real_, Rd) ) tba(head(base_FH %&gt;% select(id_dominio,n,n_upm,Rd, Rd_var,hat_var:n_eff_FGV), 10)) id_dominio n n_upm Rd Rd_var hat_var deff_FGV n_eff_FGV 0101 2951 127 0.4147 0.0005 0.0005 5.9860 492.9810 0201 221 7 0.4526 0.0024 0.0053 4.7576 46.4517 0202 86 2 NA NA NA NA NA 0203 86 2 0.7138 0.0055 0.0101 4.2227 20.3661 0204 51 1 NA NA NA NA NA 0205 34 1 NA NA NA NA NA 0206 65 2 0.5527 0.0142 0.0133 3.4692 18.7366 0208 74 2 0.8122 0.0041 0.0097 4.6595 15.8814 0210 16 1 NA NA NA NA NA 0301 264 6 0.5668 0.0037 0.0039 4.2461 62.1742 El código anterior realiza algunas transformaciones y ajustes sobre la base de datos base_sae para obtener la base de datos base_FH. Primero, se verifica que el valor Rd_deff no sea NaN, en cuyo caso se establece en 1. A continuación, se calcula el nuevo valor del deff_FGV para cada municipio mediante la fórmula que incluye el valor de hat_var (la varianza estimada a través del modelo FGV) y los valores de Rd_var y Rd_deff (la varianza de diseño y el DEFF estimado). Si Rd_var es 0, entonces se establece deff_FGV en 1. Luego, se utiliza el criterio MDS (Minimum Detectable Size) para regularizar el valor de deff_FGV, estableciendo en NA aquellos valores que son menores o iguales a 1. A partir de deff_FGV, se calcula el número efectivo de personas encuestadas n_eff_FGV. Finalmente, se excluyen aquellos municipios para los cuales no se pudo estimar la varianza a través del modelo FGV, estableciendo en NA los valores de hat_var y Rd. El resultado es una base de datos que contiene información relevante para la construcción del modelo FH. "],["otras-validaciones-sobre-el-resultado-del-modelo..html", "5.7 Otras validaciones sobre el resultado del modelo.", " 5.7 Otras validaciones sobre el resultado del modelo. Continuando con el proceso de validación se construye el siguiente gráfico de dispersión con la variable de la varianza del estimador directo en el eje y y la varianza FGV en el eje x, para los municipios que tienen valores válidos para ambas variables. La línea de regresión lineal se ajusta a los puntos usando el método de mínimos cuadrados. La visualización del gráfico permite evaluar si la FGV está capturando adecuadamente la variabilidad de la variable de interés (en este caso, la variable de varianza del estimador directo). Si la FGV captura la variabilidad, se espera que los puntos estén relativamente cerca de la línea de regresión, lo que indicaría que la FGV explica una gran parte de la variabilidad de la varianza del estimador directo. Por otro lado, si la FGV no captura la variabilidad, los puntos estarán más dispersos y alejados de la línea de regresión. nDom &lt;- sum(!is.na(base_FH$hat_var)) temp_FH &lt;- base_FH %&gt;% filter(!is.na(hat_var)) ggplot(temp_FH %&gt;% arrange(n), aes(x = hat_var, y = Rd_var)) + geom_point() + geom_smooth(method = &quot;lm&quot;, col = 2) + labs(x = &quot;FGV&quot;, y = &quot;VaRdirEst&quot;) + ylab(&quot;Varianza del Estimador Directo&quot;) Ahora, se realiza la comparación de la variabilidad de la varianza del estimador directo frente a la varianza suavizada a medida que el tamaño de muestra aumenta. El eje x representa el tamaño de la muestra y el eje y representa las varianzas. La línea azul representa la varianza FGV, mientras que la línea roja representa la varianza del estimador directo. En el gráfica es posible notar que la varianza FGV tiene una menos volatilidad que la varianza directa. ggplot(temp_FH %&gt;% arrange(n), aes(x = 1:nDom)) + geom_line(aes(y = Rd_var, color = &quot;VarDirEst&quot;)) + geom_line(aes(y = hat_var, color = &quot;FGV&quot;)) + labs(y = &quot;Varianzas&quot;, x = &quot;Tamaño muestral&quot;, color = &quot; &quot;) + scale_x_continuous(breaks = seq(1, nDom, by = 10), labels = temp_FH$n[order(temp_FH$n)][seq(1, nDom, by = 10)]) + scale_color_manual(values = c(&quot;FGV&quot; = &quot;Blue&quot;, &quot;VarDirEst&quot; = &quot;Red&quot;)) Siguiendo en la misma linea se realiza la comparación del n efectivo directo (n_eff_DIR) y el n efectivo FGV (n_eff_DIR). El código que se muestra acontinuación produce un gráfico que compara el tamaño de muestra efectivo obtenido a través de la estimación del DEFF con el tamaño de muestra directo. En el eje x se muestra el tamaño de muestra directo (n) y en el eje y se muestra el tamaño de muestra efectivo, calculado a través de la fórmula n/DEFF para la estimación directa (rojo) y para la FGV (azul). Se puede observar que, en general, el tamaño de muestra efectivo estimado a través de la FGV es menos variable que el estimado a través de la estimación directa, lo que indica que la FGV reduce la varianza de la estimación. Además, se puede observar que para algunos dominios el tamaño de muestra efectivo estimado a través de la FGV es menor que el tamaño de muestra directo, lo que puede deberse a que la estimación de la varianza a través de la FGV. En general, este gráfico es útil para comparar la eficiencia de la estimación a través de la FGV y la estimación directa para cada dominio. ggplot(temp_FH %&gt;% arrange(n), aes(x = 1:nDom)) + geom_line(aes(y = n / Rd_deff, color = &quot;n_eff_DIR&quot;)) + geom_line(aes(y = n_eff_FGV, color = &quot;n_eff_FGV&quot;)) + labs(y = &quot;Tamaño de muestra efectivo&quot;, x = &quot;Tamaño muestral&quot;, color = &quot; &quot;) + scale_x_continuous(breaks = seq(1, nDom, by = 10), labels = temp_FH$n[order(temp_FH$n)][seq(1, nDom, by = 10)]) + scale_color_manual(values = c(&quot;n_eff_FGV&quot; = &quot;Blue&quot;, &quot;n_eff_DIR&quot; = &quot;red&quot;)) Guardando el archivo saveRDS(object = base_FH, &quot;../Data/base_FH.Rds&quot;) "],["estimación-del-modelo-de-área.html", "Capítulo 6 Estimación del modelo de área", " Capítulo 6 Estimación del modelo de área El modelo de Fay Herriot FH, propuesto por Fay y Herriot (1979), es un modelo estadístico de área y es el más comúnmente utilizado, cabe tener en cuenta, que dentro de la metodología de estimación en áreas pequeñas, los modelos de área son los de mayor aplicación, ya que lo más factible es no contar con la información a nivel de individuo, pero si encontrar no solo los datos a nivel de área, sino también información auxiliar asociada a estos datos. Este modelo lineal mixto, fue el primero en incluir efectos aleatorios a nivel de área, lo que implica que la mayoría de la información que se introduce al modelo corresponde a agregaciaciones usualmente, departamentos, regiones, provincias, municipios entre otros, donde las estimaciones que se logran con el modelo se obtienen sobre estas agregaciones o subpoblaciones. Además, el modelo FH utiliza información auxiliar para mejorar la precisión de las estimaciones en áreas pequeñas. Esta información auxiliar puede ser de diferentes tipos, como censos poblacionales, encuestas de hogares, registros administrativos, entre otros. La inclusión de esta información auxiliar se realiza a través de un modelo lineal mixto, en el que se consideran tanto efectos fijos como aleatorios a nivel de área. Los efectos fijos representan la relación entre la variable de interés y las variables auxiliares, mientras que los efectos aleatorios capturan la variabilidad no explicada por estas variables. De esta forma, el modelo FH permite obtener estimaciones precisas y confiables para subpoblaciones o áreas pequeñas, lo que resulta de gran utilidad para la toma de decisiones en diferentes ámbitos, como políticas públicas, planificación urbana, entre otros. "],["lectura-de-librerías-2.html", "6.1 Lectura de librerías", " 6.1 Lectura de librerías library(survey) library(srvyr) library(TeachingSampling) library(stringr) library(magrittr) library(sae) library(ggplot2) library(emdi) library(patchwork) library(readxl) library(tidyverse) select &lt;- dplyr::select id_dominio &lt;- &quot;id_dominio&quot; "],["lectura-de-bases-de-datos.html", "6.2 Lectura de bases de datos", " 6.2 Lectura de bases de datos Para la lecturas de las insumos se emplean los siguientes comando de R. La función readRDS() es utilizada para cargar objetos creados previamente y guardados en archivos RDS. Los archivos RDS contienen objetos de R guardados de forma binaria, lo que permite guardar y recuperar objetos de R con facilidad. En este caso, se cargan tres objetos diferentes: base_FH,statelevel_predictorsyDEE_Mun`. # Estimación Directa + FGV base_FH &lt;- readRDS(&#39;../Data/base_FH.Rds&#39;) # Variables predicadoras statelevel_predictors &lt;- readRDS(&quot;../Data/statelevel_predictors_df.rds&quot;) DEE_Mun &lt;- readRDS(&#39;../Data/DEE_Mun.Rds&#39;) "],["consolidando-las-bases-de-datos..html", "6.3 Consolidando las bases de datos.", " 6.3 Consolidando las bases de datos. La función select() del paquete dplyr se utiliza para seleccionar columnas de un data.frame. En este caso, se seleccionan las columnas id_dominio, Rd, hat_var, n_eff_FGV y n de la base de datos base_FH. Luego, se utiliza la función full_join() de dplyr para unir los datos de la base de datos base_FH con los datos de la base de datos statelevel_predictors por la columna id_dominio. La opción by se utiliza para especificar la columna por la cual se realizará la unión. Por último, se utiliza la función left_join() de dplyr para unir los datos de la base de datos base_completa con los datos de la base de datos DEE_Mun por la columna id_dominio. Al igual que en el caso anterior, la opción by se utiliza para especificar la columna por la cual se realizará la unión. base_completa &lt;- base_FH %&gt;% select(id_dominio, Rd, hat_var, n_eff_FGV,n) %&gt;% full_join(statelevel_predictors, by = id_dominio) base_completa &lt;- left_join(base_completa, DEE_Mun, by = id_dominio) Fialmente se almacena la base consolidada en un archivo .rds saveRDS(base_completa, &#39;Data/base_completa.Rds&#39;) "],["estimando-el-modelo-de-área-con-transformación-arcoseno.html", "6.4 Estimando el modelo de área con transformación arcoseno", " 6.4 Estimando el modelo de área con transformación arcoseno El código corresponde a la estimación del modelo de Fay Herriot (FH) utilizando el método de mínimos cuadrados restringidos (REML) y transformación de los datos mediante la función arcsin. En la especificación del modelo se definen las variables predictoras fijas y la variable dependiente (Rd), que es la estimación directa en una determinada área (dominio). Además, se especifica la variable auxiliar para la estimación de varianza (hat_var). Se utiliza la base de datos completa (base_completa) que incluye la información de las variables predictoras a nivel de dominio, información auxiliar y el tamaño de muestra efectivo para cada dominio (eff_smpsize). Se especifica que el método de estimación de la varianza es mediante bootstrapping (mse_type = \"boot\") con un número de replicaciones igual a 500 (B = 500). El modelo también incluye efectos aleatorios a nivel de dominio, lo que se indica mediante la especificación de los dominios (domains). Finalmente, se especifica que se desea la transformación de los datos mediante la función arcsin (transformation = \"arcsin\") y su backtransformation mediante el método bias-corrected (backtransformation = \"bc\"). Se utiliza el parámetro MSE = TRUE para obtener la estimación de la varianza del error de muestreo (MSE) del estimador. fh_arcsin &lt;- fh( fixed = Rd ~ 0 + P45_TUVO_EMPLEO + P46_ACTIVIDAD_PORPAGA + P47_AYUDO_SINPAGA + P30_DONDE_NACE + P41_ANOS_UNIVERSITARIOS + P38_ANOEST + P44_PAIS_VIVIA_CODIGO + P50_DISPUESTO_TRABAJAR + P51_TRABAJO_ANTES + P40_SEGRADUO + P29_EDAD_ANOS_CUMPLIDOS + P35_SABE_LEER + P27_SEXO + H25C_TOTAL + P45R1_CONDICION_ACTIVIDAD_OCUPADO + P45R1_CONDICION_ACTIVIDAD_DISCAPACITADO + P45R1_CONDICION_ACTIVIDAD_1erTrabajo + P45R1_CONDICION_ACTIVIDAD_EDUCACION + P54R1_CATOCUP_SINCOM + P27_SEXO_JEFE + P29_EDAD_JEFE + ZONA_Rur + H31_HACINAMIENTO + H15_PROCEDENCIA_AGUA + H17_ALUMBRADO + H12_SANITARIO + V03_PAREDES + V04_TECHO + V05_PISO + H32_GRADSAN + H35_GRUPSEC + luces_nocturnas_promedio + cubrimiento_cultivo_suma + cubrimiento_urbano_suma + accesibilidad_hospitales_promedio + accesibilidad_hosp_caminado_promedio, vardir = &quot;hat_var&quot;, combined_data = base_completa %&gt;% data.frame(), domains = id_dominio, method = &quot;reml&quot;, transformation = &quot;arcsin&quot;, backtransformation = &quot;bc&quot;, eff_smpsize = &quot;n_eff_FGV&quot;, MSE = TRUE, mse_type = &quot;boot&quot;, B = 500 ) leer el modelo compilado previamente. fh_arcsin &lt;- readRDS(&#39;../Data/fh_arcsin.Rds&#39;) "],["estimaciones-de-los-dominios-a-partir-del-modelo..html", "6.5 Estimaciones de los dominios a partir del modelo.", " 6.5 Estimaciones de los dominios a partir del modelo. Lo primero que hace el código es obtener los valores de gamma (\\(\\gamma\\)), recordemos que el estimador de FH esta dado como una combinación convexa entre la estimación directa y la estimación sintética, es decir, \\(\\tilde{\\theta}_{d}^{FH} = \\hat{\\gamma_d}\\hat{\\theta}^{DIR}_{d}+(1-\\hat{\\gamma_d})\\boldsymbol{x_d}^{T}\\hat{\\boldsymbol{\\beta}}\\). Luego, se utiliza la función estimators() para calcular las predicciones de las estimaciones directas y sus errores estándar, tanto de forma puntual como mediante un intervalo de confianza. Finalmente, se juntan los resultados con información auxiliar sobre los dominios, como su tamaño muestral y el valor del Gamma, y se imprime una tabla con las primeras 20 filas de la tabla de resultados. data_Gamma &lt;- fh_arcsin$model$gamma %&gt;% transmute(id_dominio = Domain, Gamma) estimaciones &lt;- estimators(fh_arcsin, indicator = &quot;All&quot;, MSE = TRUE, CV = TRUE) %&gt;% as.data.frame() %&gt;% rename(id_dominio = Domain) %&gt;% left_join(base_completa %&gt;% transmute(id_dominio, n = ifelse(is.na(n), 0, n)), by = id_dominio) %&gt;% left_join(data_Gamma, by = id_dominio) %&gt;% dplyr::select(id_dominio, everything()) tba(head(estimaciones, 20)) id_dominio Direct Direct_MSE Direct_CV FH FH_MSE FH_CV n Gamma 0101 0.4147 0.0005 0.0534 0.4142 0.0004 0.0496 2951 0.1207 0201 0.4526 0.0053 0.1613 0.5390 0.0017 0.0766 221 0.0128 0202 NA NA NA 0.6466 0.0051 0.1104 86 NA 0203 0.7138 0.0101 0.1406 0.7917 0.0029 0.0682 86 0.0056 0204 NA NA NA 0.7472 0.0053 0.0972 51 NA 0205 NA NA NA 0.7119 0.0104 0.1431 34 NA 0206 0.5527 0.0133 0.2088 0.5870 0.0041 0.1087 65 0.0052 0208 0.8122 0.0097 0.1210 0.6703 0.0060 0.1156 74 0.0044 0210 NA NA NA 0.7081 0.0108 0.1465 16 NA 0301 0.5668 0.0039 0.1100 0.5305 0.0013 0.0683 264 0.0170 0302 0.7561 0.0059 0.1017 0.6841 0.0040 0.0921 123 0.0085 0303 0.6078 0.0047 0.1125 0.6168 0.0012 0.0572 206 0.0139 0304 0.6450 0.0051 0.1110 0.7021 0.0025 0.0717 176 0.0121 0305 NA NA NA 0.6842 0.0053 0.1066 51 NA 0401 0.5419 0.0021 0.0840 0.5609 0.0012 0.0627 481 0.0319 0402 NA NA NA 0.7460 0.0047 0.0916 75 NA 0403 0.6788 0.0083 0.1345 0.6959 0.0036 0.0859 108 0.0072 0404 NA NA NA 0.5999 0.0057 0.1257 68 NA 0405 0.5383 0.0050 0.1309 0.5777 0.0026 0.0889 221 0.0136 0407 0.7513 0.0124 0.1482 0.6447 0.0046 0.1050 67 0.0042 De la Tabla es posible notar que las estimaciones de FH existen para todos los dominios. guardar estimaciones saveRDS(estimaciones, &#39;../Data/estimaciones.Rds&#39;) "],["validación-del-modelo-de-área.html", "Capítulo 7 Validación del modelo de área ", " Capítulo 7 Validación del modelo de área "],["lectura-de-librerías-3.html", "7.1 Lectura de librerías", " 7.1 Lectura de librerías library(tidyverse) library(survey) library(srvyr) library(TeachingSampling) library(stringr) library(magrittr) library(sae) library(ggplot2) library(emdi) library(patchwork) library(reshape2) library(haven) library(BayesSAE) library(mice) library(rgdal) library(spdep) library(tmap) library(sp) library(sf) library(dplyr) library(magrittr) select &lt;- dplyr::select "],["leer-bases-de-datos.html", "7.2 Leer bases de datos", " 7.2 Leer bases de datos El código lee varios archivos y realiza algunas transformaciones en una base de datos de estimaciones de la tasa de informalidad en diferentes dominios geográficos. Primero, se lee un archivo Shapefile llamado DOM.shp que contiene la información geográfica de los polígonos de los diferentes dominios. Luego, se lee una base de datos completa de donde se extraen las estimaciones de la tasa de informalidad en los diferentes dominios. A continuación, se lee un modelo FH (Fay-Herriot) con transformación Arcoseno, que es utilizado para estimar las tasas de informalidad en los diferentes dominios geográficos. También se leen las estimaciones del modelo FH con transformación Arcoseno y se calculan algunas medidas de error como el RMSE (Root Mean Squared Error), RRMSE (Relative Root Mean Squared Error) y la desviación estándar del error de la estimación Directa. # Shapefile poligonos_dominio &lt;- read_sf(&quot;../shapefiles2010/DOM.shp&quot; ) # Base de datos completa base_completa &lt;- readRDS(&#39;../Data/base_completa.Rds&#39;) # Modelo FH con transformación Arcoseno fh_arcsin &lt;- readRDS(&quot;../Data/fh_arcsin.Rds&quot;) # Estimaciones del modelo FH con transformación Arcoseno estimaciones &lt;- readRDS(&#39;../Data/estimaciones.Rds&#39;) %&gt;% mutate(rrmse_FH = sqrt(FH_MSE) / FH, rmse_FH = sqrt(FH_MSE), Direct_ee = sqrt(Direct_MSE)) "],["análisis-de-residuales-y-efectos-aleatorios.html", "7.3 Análisis de residuales y efectos aleatorios", " 7.3 Análisis de residuales y efectos aleatorios El código muestra cómo calcular y graficar los residuos estandarizados de un modelo de FH de datos transformados con la función arcoseno. En primer lugar, se calculan los residuos estandarizados del modelo, utilizando la fórmula (valor observado - valor predicho) / desviación estándar de los residuos. Luego, se calculan los residuos estandarizados de los efectos aleatorios del modelo, utilizando la misma fórmula pero con los efectos aleatorios en lugar de los residuos. Finalmente, se grafican los residuos estandarizados de los datos utilizando ggplot2. Se muestra un gráfico de puntos de los residuos estandarizados en función del índice de observación. También se traza una línea horizontal en el valor cero para facilitar la identificación de puntos que estén lejos de cero. #--- Residuales estandarizados ---# residuals &lt;- fh_arcsin$model$real_residuals std_residuals &lt;- (residuals - mean(residuals)) / sd(residuals) #--- Residuales estandarizados de los efectos aleatorios ---# rand.eff &lt;- fh_arcsin$model$random_effects srand.eff &lt;- (rand.eff - mean(rand.eff)) / sd(rand.eff) #--- Gráfico de residuales estandarizados ---# theme_set(theme_bw()) ggplot(data.frame(Residuals = fh_arcsin$model$std_real_residuals)) + geom_point(aes(y = Residuals, x = 1:length(Residuals))) + labs(y = &quot;Residuales estandarizados&quot;, x = &quot;&quot;) + geom_hline(yintercept = 0, col = &quot;blue&quot;) 7.3.1 QQ plots Este código genera dos gráficos QQ (quantile-quantile) para evaluar si los residuos y los efectos aleatorios del modelo FH con transformación Arcoseno siguen una distribución normal. Primero, se calculan los residuos estandarizados y los efectos aleatorios estandarizados dividiéndolos por su desviación estándar y restando su media. Luego, se crea el primer gráfico QQ utilizando ggplot2, que muestra una comparación de la distribución empírica de los residuos estandarizados con una distribución normal teórica. El segundo gráfico QQ realiza la misma comparación, pero para los efectos aleatorios estandarizados. Finalmente, se utiliza el operador | para unir los dos gráficos QQ en una única visualización. p1 &lt;- ggplot(data.frame(Residuals = std_residuals), aes(sample = Residuals)) + stat_qq() + stat_qq_line(col = &quot;blue&quot;) + ggtitle(&quot;qqplot - residuales estandarizados&quot;) p2 &lt;- ggplot(data.frame(Residuals = srand.eff), aes(sample = Residuals)) + stat_qq() + stat_qq_line(col = &quot;blue&quot;) + ggtitle(&quot;qqplot - efectos aleatorios&quot;) p1 | p2 7.3.2 Densidades Este código genera dos gráficos de densidad para los residuales estandarizados y los efectos aleatorios estandarizados del modelo FH con transformación Arcoseno. En cada gráfico se utiliza la función ggplot para crear un objeto de gráfico y se establecen los datos, el eje x y los colores de las líneas y relleno de los gráficos de densidad. Para el gráfico de densidad de los residuales estandarizados (p3), se establece una distribución normal teórica (con la función stat_function) y se define un rango de -4 a 4 en el eje x (con xlim). El color de relleno y línea se establece como azul claro (lightblue3) con un nivel de transparencia de 0.4 (alpha). En el gráfico de densidad de los efectos aleatorios estandarizados (p4), se establece también una distribución normal teórica y un rango de -4 a 4 en el eje x, pero se utiliza el mismo color de relleno y línea que en el gráfico de densidad de los residuales estandarizados (color[2]). Ambos gráficos se combinan utilizando el operador “|” para presentarse en la misma línea. color = c(&quot;blue&quot;, &quot;lightblue3&quot;) p3 &lt;- ggplot( data.frame(Residuals = std_residuals), aes(x = Residuals), fill = color[2], color = color[2] ) + geom_density(fill = color[2], color = color[2], alpha = 0.4) + stat_function(fun = dnorm) + xlim(-4, 4) + ggtitle(&quot;Densidad - residuales estandarizados&quot;) p4 &lt;- ggplot( data.frame(Residuals = srand.eff), aes(x = Residuals), fill = color[2], color = color[2] ) + geom_density(fill = color[2], color = color[2], alpha = 0.4) + stat_function(fun = dnorm) + xlim(-4, 4) + ggtitle(&quot;Densidad - efectos aleatorios&quot;) p3 | p4 a &lt;- summary(fh_arcsin) a$normality %&gt;% tba() Skewness Kurtosis Shapiro_W Shapiro_p Standardized_Residuals 0.2087 2.7168 0.9890 0.6659 Random_effects 0.2712 2.8440 0.9902 0.7491 "],["coeficiente-de-determinación.html", "7.4 Coeficiente de determinación", " 7.4 Coeficiente de determinación Este fragmento de código ajusta un modelo FH y calcula el coeficiente de determinación (R2) del modelo. Primero, filtra las observaciones de estimaciones que no tienen valores nulos en la columna Gamma. Luego, une los datos filtrados con la base de datos completa base_completa mediante una operación de inner_join(). A continuación, se obtienen los coeficientes del modelo ajustado fh_arcsin y se almacenan en una matriz llamada Betas. También se crea una matriz XS que contiene las variables independientes de la regresión para cada observación de base_dir. El código luego calcula los residuos del modelo utilizando la matriz XS y Betas. El error cuadrático medio (MSE) se estima a partir de los residuos y se almacena en S2Beta. La variable su2 contiene la varianza residual del modelo FH ajustado y se utiliza para calcular el coeficiente de determinación (R2) del modelo. Finalmente, el código muestra el valor de R2 calculado. dom_obs &lt;- estimaciones %&gt;% filter(!is.na(Gamma)) base_dir &lt;- base_completa %&gt;% inner_join(x = dom_obs) #--- Coeficientes del modelo ajustado ---# Betas &lt;- as.matrix(fh_arcsin$model$coefficients[, 1], ncol = 1) rownames(Betas) &lt;- rownames(fh_arcsin$model$coefficients) XS &lt;- cbind(as.matrix(base_dir %&gt;% dplyr::select(rownames(Betas)))) residuos &lt;- XS %*% Betas - c(colMeans(XS) %*% Betas) D &lt;- dim(XS)[1] q &lt;- dim(XS)[2] S2Beta &lt;- sum(residuos ^ 2) / (D - 1) su2 &lt;- fh_arcsin$model$variance (R2 &lt;- 1 - (su2 / (((D - q) / (D - 1)) * su2 + S2Beta))) ## [1] 0.9927712 "],["validaciones-en-relación-al-tamaño-de-la-muestra.html", "7.5 Validaciones en relación al tamaño de la muestra", " 7.5 Validaciones en relación al tamaño de la muestra Este código genera un gráfico que compara las estimaciones obtenidas a partir de dos métodos diferentes (“Directo” y “Fay Herriot”) en función del tamaño muestral de los dominios. Primero, se define el número de dominios como la cantidad de filas en base_completa. Luego, se utiliza ggplot() para generar el gráfico. Se utiliza geom_line() para trazar una línea para cada uno de los dos métodos, especificando los colores correspondientes (“Directo” en azul y “Fay Herriot” en rojo) en scale_color_manual(). El eje x se etiqueta con el tamaño muestral de los dominios y el eje y se etiqueta como “Formalidad”. La escala del eje x se personaliza para que los intervalos sean de 10 unidades, con las etiquetas correspondientes obtenidas de estimaciones$n. N_dominios &lt;- nrow(base_completa) ggplot(estimaciones[order(estimaciones$n), ], aes(x = 1:N_dominios)) + geom_line(aes(y = Direct, color = &quot;Directo&quot;)) + geom_line(aes(y = FH, color = &quot;Fay Herriot&quot;)) + labs(y = &quot;Formalidad&quot;, x = &quot;Tamaño muestral&quot;, color = &quot;&quot;) + scale_x_continuous(breaks = seq(1, N_dominios, by = 10), labels = estimaciones$n[order(estimaciones$n)][seq(1, N_dominios, by = 10)]) + scale_color_manual(values = c(&quot;Directo&quot; = &quot;Blue&quot;, &quot;Fay Herriot&quot; = &quot;red&quot;)) Estimación directa vs Fay-Herriot Este código crea un gráfico de dispersión con una línea de ajuste lineal para comparar las estimaciones obtenidas mediante la estimación directa y la estimación de Fay-Herriot La función ggplot() recibe como argumento el conjunto de datos estimaciones y define la variable Direct para el eje x y la variable FH para el eje y. geom_point() agrega los puntos correspondientes a cada par de estimaciones para ambas variables. geom_smooth() agrega una línea de ajuste lineal a los puntos utilizando el método “lm” (mínimos cuadrados) por defecto de ggplot(). ggtitle agrega un título al gráfico indicando la comparación de las estimaciones. ggplot(estimaciones, aes(Direct, FH)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + ggtitle(&quot;Comparación de estimaciones&quot;) 7.5.1 Distancias de Cook El código calcula la contribución de cada dominio a la varianza del estimador directo en un diseño de muestreo complejo usando el método de Fay-Herriot. Primero, se define el tamaño de la población D y se crea un vector CD de longitud D con valores iniciales igual a cero. Luego, se extraen los coeficientes del modelo de regresión lineal ajustado usando el método de Fay-Herriot y se almacenan en Betas. A continuación, se crea una matriz diagonal V con los valores de la varianza de los errores de muestreo obtenidos del modelo de Fay-Herriot ajustado. El ciclo for itera a través de cada dominio i y ajusta un modelo Fay-Herriot sin ese dominio. Luego, se calculan los coeficientes ajustados del modelo sin el dominio i y se almacenan en Betas_i. Se calcula la diferencia entre los coeficientes ajustados de Betas y Betas_i y se almacena en betaDiff. Finalmente, se calcula la contribución del dominio i a la varianza del estimador directo multiplicando betaDiff por la matriz t(XS) %*% V %*% XS y dividiendo el resultado por (q - 1), donde XS es la matriz de diseño, V es la matriz de varianza, q es el número de variables explicativas y betaDiff es la diferencia entre los coeficientes ajustados del modelo completo y el modelo sin el dominio i. La contribución se almacena en la posición i del vector CD. D &lt;- dim(base_completa)[1] CD = numeric(D) Betas &lt;- as.matrix(fh_arcsin$model$coefficients[, 1], ncol = 1) V &lt;- diag(1 / (su2 + 1 / (4 * base_dir$n_eff_FGV))) #--- Ciclo para DC ---# for (i in 1:D) { print(i) BetaModelo &lt;- fh(formula(fh_arcsin$call$fixed), vardir = &quot;hat_var&quot;, combined_data = base_completa[-i, ] %&gt;% data.frame(), domains = &quot;id_dominio&quot;, method = &quot;reml&quot;, transformation = &quot;arcsin&quot;, backtransformation = &quot;bc&quot;, eff_smpsize = &quot;n_eff_FGV&quot;, MSE = FALSE, mse_type = &quot;NULL&quot; ) #--- Coeficientes del modelo ajustado ---# Betas_i = as.matrix(BetaModelo$model$coefficients[, 1], ncol = 1) betaDiff &lt;- Betas - Betas_i CD[i] = (1 / (q - 1)) * t(betaDiff) %*% (t(XS) %*% V %*% XS) %*% betaDiff } lectura de la distancias de Cook calculadas previamente CD &lt;- readRDS(&quot;../Data/cookDis_2021.rds&quot;) data.frame(cookDis = CD, dominios = base_completa$DES_MUNICIPIO) %&gt;% filter(cookDis &gt; 0.1) %&gt;% tba() cookDis dominios 0.2207 JIMANI 0.6132 HIGÜEY 0.2151 LAS TERRENAS 1.9380 SANTO DOMINGO NORTE Gráfico de Distancias de Cook data.frame(cookDis = CD, dominios = base_completa$DES_MUNICIPIO) %&gt;% ggplot(aes(y = cookDis, x = 1:N_dominios)) + geom_point(col = &quot;blue&quot;) + geom_text(aes(label = ifelse(cookDis &gt; 0.1, as.character(dominios), &#39;&#39;)), hjust = 0, vjust = 0) + labs(y = &quot;Distancia de Cook&quot;, x = &quot;Municipios&quot;) "],["análisis-gráfico.html", "7.6 Análisis gráfico", " 7.6 Análisis gráfico 7.6.1 Coeficiente de variación y RRMSE Este código genera un gráfico de líneas comparando los coeficientes de variación (CV) del estimador directo y del estimador FH en función del tamaño muestral de los dominios. El eje x muestra el tamaño muestral y el eje y los valores de los coeficientes de variación. El gráfico tiene dos líneas: una para el CV del estimador directo y otra para el RRMSE del estimador FH. Los puntos se conectan mediante una línea recta para una mejor visualización. El código comienza filtrando los datos de estimaciones para ordenarlos por el tamaño muestral n. Luego, se utiliza la función ggplot() para crear el objeto de la gráfica. Se especifican los datos y las variables que se utilizarán en la gráfica (aes(x = 1:N_dominios) y aes(y = Direct_CV, color = \"CV\") y aes(y = rrmse_FH, color = \"RRMSE\")). Se añaden dos líneas con geom_line() correspondientes a cada estimador, y se establece el título del gráfico con ggtitle(). Se establecen las etiquetas de los ejes x e y mediante labs() y se definen los valores que aparecerán en el eje x con scale_x_continuous(). Finalmente, se establecen los colores de las líneas con scale_color_manual(). ggplot(estimaciones %&gt;% arrange(n), aes(x = 1:N_dominios)) + geom_line(aes(y = Direct_CV, color = &quot;CV&quot;)) + geom_line(aes(y = rrmse_FH, color = &quot;RRMSE&quot;)) + labs(y = &quot;Coeficientes de variación&quot;, x = &quot;Tamaño muestral&quot;, color = &quot;&quot;) + scale_x_continuous(breaks = seq(1, N_dominios, by = 10), labels = estimaciones$n[order(estimaciones$n)][seq(1, N_dominios, by = 10)]) + scale_color_manual(values = c(&quot;CV&quot; = &quot;Blue&quot;, &quot;RRMSE&quot; = &quot;red&quot;)) 7.6.2 Error estándar y RRMSE ggplot(estimaciones %&gt;% arrange(n), aes(x = 1:N_dominios)) + geom_line(aes(y = Direct_ee, color = &quot;Directo&quot;)) + geom_line(aes(y = rmse_FH, color = &quot;Fay-Herriot&quot;)) + labs(y = &quot;Error estándar&quot;, x = &quot;Tamaño muestral&quot;, color = &quot;&quot;) + scale_x_continuous(breaks = seq(1, N_dominios, by = 10), labels = estimaciones$n[order(estimaciones$n)][seq(1, N_dominios, by = 10)]) + scale_color_manual(values = c(&quot;Directo&quot; = &quot;Blue&quot;, &quot;Fay-Herriot&quot; = &quot;red&quot;)) 7.6.3 Boxplot: CV Directo estimaciones %&gt;% ggplot(aes(y =Direct_CV)) + geom_boxplot() 7.6.4 Boxplot: CV Directo y RRMSE FH (observados) melted &lt;- estimaciones %&gt;% filter(!is.na(Direct_CV)) %&gt;% dplyr::select(Direct_CV, rrmse_FH) %&gt;% melt() ggplot(melted, aes(factor(variable), value)) + geom_boxplot() + labs(y = &quot;Coeficiente de variación&quot;, x = &quot;&quot;) 7.6.5 Boxplot: CV Directo y RRMSE FH melted &lt;- estimaciones %&gt;% dplyr::select(Direct_CV, rrmse_FH) %&gt;% melt() ggplot(melted, aes(factor(variable), value)) + geom_boxplot() + labs(y = &quot;Coeficiente de variación&quot;, x = &quot;&quot;) 7.6.6 Boxplot: Error estándar Directo y RMSE FH (observados) melted &lt;- estimaciones %&gt;% filter(!is.na(Direct_CV)) %&gt;% dplyr::select(Direct_ee, rmse_FH) %&gt;% melt() ggplot(melted, aes(factor(variable), value)) + geom_boxplot() + labs(y = &quot;Errores estándar&quot;, x = &quot;&quot;) 7.6.7 Boxplot: Error estándar Directo y RMSE FH melted &lt;- estimaciones %&gt;% dplyr::select(Direct_ee, rmse_FH) %&gt;% melt() ggplot(melted, aes(factor(variable), value)) + geom_boxplot() + labs(y = &quot;Errores estándar&quot;, x = &quot;&quot;) "],["matriz-vecinos-cercanos.html", "7.7 Matriz vecinos cercanos", " 7.7 Matriz vecinos cercanos #--- Construyendo la matriz de vecinos de una lista de polígonos ---# lista_comvecinos &lt;- spdep::poly2nb(pl = poligonos_dominio, queen = FALSE) Cargando: Matriz de vecinos lista_comvecinos &lt;- readRDS(&quot;../Data/lista_comvecinos.rds&quot;) #--- Matrices de ponderaciones espaciales para matriz de vecinos ---# W &lt;- nb2mat(lista_comvecinos, zero.policy = T) #--- Nombrando las filas y columnas de la matriz de ponderaciones espaciales --# row.names(W) &lt;- paste0(poligonos_dominio$id_dominio) colnames(W) &lt;- paste0(poligonos_dominio$id_dominio) #--- Ordenando las filas y columnas de la matriz de ponderaciones espaciales --# W &lt;- W[order(row.names(W)),] W &lt;- W[, order(colnames(W))] #---- Comunas que se encuentran en la matriz de pesos W y de estimaciones ---# fil &lt;- row.names(W) %in% estimaciones$id_dominio[!is.na(estimaciones$FH)] W2 &lt;- W[fil, fil] #--- Pruebas de correlación espacial de Moran&#39;s I y Geary&#39;s C ---# spatialcor.tests(estimaciones$FH[fil], W2) %&gt;% tba() Statistics Value p.value Moran’s I 0.0992 0.0209 Geary’s C 0.8567 0.0055 "],["benchmark-regional-para-las-estimaciones-del-modelo-de-área.html", "Capítulo 8 Benchmark regional para las estimaciones del modelo de área ", " Capítulo 8 Benchmark regional para las estimaciones del modelo de área "],["lectura-de-librerías-4.html", "8.1 Lectura de librerías", " 8.1 Lectura de librerías library(survey) library(srvyr) library(TeachingSampling) library(stringr) library(magrittr) library(sae) library(ggplot2) library(emdi) library(patchwork) library(DBI) library(odbc) library(flexmix) library(modeltools) library(sp) library(sf) library(rgdal) library(tmap) library(dplyr) id_dominio &lt;- &quot;id_dominio&quot; "],["lectura-de-bases-de-datos-1.html", "8.2 Lectura de bases de datos", " 8.2 Lectura de bases de datos El código está leyendo varios archivos necesarios para el análisis. poligonos_dominios: Es un archivo shapefile que contiene la información geográfica (polígonos) de los dominios de la encuesta. Se carga usando la función read_sf() del paquete sf. base_completa: Es un archivo RDS que contiene la información de la encuesta. Incluye información de los dominios, los estratos, los conglomerados y los pesos. estimacionesPre: Es un archivo RDS que contiene las estimaciones directas de los dominios. base_FH: Es una base que se crea al juntar la base_completa con las estimacionesPre usando la función left_join() del paquete dplyr. encuesta: Es un archivo RDS que contiene la información de la encuesta. fh_arcsin: Es un archivo RDS que contiene las estimaciones del modelo ajustado de Fay-Herriot. personas_dominio: Es un archivo RDS que contiene número de personas en cada dominio. personas_dominio_agregado: Es un archivo RDS que contiene el número total de personas agregada por región. ## Shapefile poligonos_dominios &lt;- read_sf( &quot;../shapefiles2010/DOM.shp&quot;) ## Base de estimaciones base_completa &lt;- readRDS(&#39;../Data/base_completa.Rds&#39;) estimacionesPre &lt;- readRDS(&#39;../Data/estimaciones.Rds&#39;) base_FH &lt;- left_join(base_completa, estimacionesPre, by = id_dominio) ## Encuesta encuesta &lt;- readRDS(&quot;../Data/encuestaDOM.Rds&quot;) ## Estimaciones del modelo ajustado fh_arcsin &lt;- readRDS(&quot;../Data/fh_arcsin.Rds&quot;) ## Agregados por dominios y región personas_dominio &lt;- readRDS(&#39;../Data/agregado_persona_dominio.rds&#39;) personas_dominio_agregado &lt;- readRDS(&#39;../Data/agregado_persona_region.rds&#39;) "],["estimación-directa-por-región.html", "8.3 Estimación directa por región", " 8.3 Estimación directa por región El código carga y prepara los datos de la encuesta y luego calcula las estimaciones directas en cada región del país. Primero, para el conjunto de datos de la encuesta se utiliza la función mutate() de dplyr para agregar tres columnas: upm, estrato y factor_anual. La columna upm es la identificación única de la unidad primaria de muestreo, estrato es la identificación del estrato en el que se encuentra la unidad primaria de muestreo y factor_anual es el factor de expansión anual, calculado dividiendo el factor de expansión trimestral entre 4. A continuación, se utiliza la función as_survey_design() de la librería survey para crear un objeto de diseño de la encuesta a partir de los datos. La función utiliza las columnas estrato, upm y factor_anual para especificar las estrategias de muestreo, el peso de muestreo y la estructura de conglomerados. Por último, se utiliza la función group_by() de dplyr para agrupar los datos por id_region, luego se utiliza la función summarise() de dplyr junto con survey_ratio() de survey para calcular la estimación directa. El resultado se asigna a un objeto llamado directoDepto, que contiene las columnas id_region y theta_region, que corresponde a la estimación directa en cada región. encuesta &lt;- encuesta %&gt;% mutate( upm = str_pad(string = upm,width = 9,pad = &quot;0&quot;), estrato = str_pad(string = estrato,width = 5,pad = &quot;0&quot;), factor_anual = factor_expansion / 4 ) %&gt;% data.frame() disenoDOM &lt;- encuesta %&gt;% as_survey_design( strata = estrato, ids = upm, weights = factor_anual, nest=T ) directoDepto &lt;- disenoDOM %&gt;% group_by(id_region) %&gt;% filter(ocupado == 1 &amp; pet == 1) %&gt;% summarise(Rd = survey_ratio( numerator = orden_sector == 2 , denominator = 1, vartype = c(&quot;se&quot;, &quot;ci&quot;, &quot;var&quot;, &quot;cv&quot;), deff = T )) %&gt;% transmute(id_region, theta_region = Rd) "],["consolidación-de-la-base-de-datos.html", "8.4 Consolidación de la base de datos", " 8.4 Consolidación de la base de datos Este código realiza los siguientes pasos: Realiza un left_join() entre la tabla directoDepto y la tabla personas_dominio por la columna id_region. Realiza un left_join() entre la tabla resultante del paso anterior y la tabla personas_dominio_agregado por la columna id_region. El resultado de esta unión es una tabla que agrega la información por dominio a nivel de región. Realiza un left_join() entre la tabla resultante del paso anterior y la tabla estimacionesPre por la columna id_dominio. El resultado de esta unión es una tabla que agrega la información de ambas tablas por dominio. Transmuta la columna FayHerriot de la tabla resultante del paso anterior y cambia su nombre a FayHarriot. El resultado final es una tabla llamada R_mpio que contiene información agregada por región y dominio, así como las estimaciones por dominio utilizando el método de Fay Herriot. R_mpio &lt;- directoDepto %&gt;% left_join(personas_dominio, by = &#39;id_region&#39;) %&gt;% left_join(personas_dominio_agregado, by = &quot;id_region&quot;) %&gt;% left_join(estimacionesPre %&gt;% transmute(id_dominio, FayHerriot = FH), by = id_dominio) %&gt;% data.frame() "],["pesos-benchmark.html", "8.5 Pesos Benchmark", " 8.5 Pesos Benchmark Para obtener el factor multiplicador de de Benchmarking se ejecuta el código siguiente. El código está creando un objeto llamado R_mpio2, el cual se está construyendo a partir del objeto R_mpio. Primero, se agrupa por id_region usando group_by() y se calculan dos medidas de razón de la encuesta utilizando summarise(). Los cálculos de las medidas de razón no están claros ya que dependen de variables no presentes en el código. Luego, se realiza un left_join() con el objeto directoDepto para añadir la variable theta_region. Posteriormente, se construye un objeto llamado pesos a partir del objeto R_mpio utilizando ungroup() para eliminar la agrupación anteriormente realizada. Luego, se añade la columna W_i que representa los pesos muestrales utilizados para el análisis. La función tba() no está disponible en R base o en los paquetes dplyr, por lo que no es posible determinar lo que hace. Probablemente es una función personalizada por el autor del código. R_mpio2 &lt;- R_mpio %&gt;% group_by(id_region) %&gt;% summarise( R_region_RB = unique(theta_region) / sum((pp_dominio / pp_region ) * FayHerriot), R_region_DB = unique(theta_region) - sum((pp_dominio / pp_region ) * FayHerriot) ) %&gt;% left_join(directoDepto, by = &quot;id_region&quot;) pesos &lt;- R_mpio %&gt;% ungroup() %&gt;% mutate(W_i = pp_dominio / pp_region) %&gt;% dplyr::select(id_dominio, W_i) head(pesos, 15) %&gt;% tba() id_dominio W_i 0101 0.2890 3201 0.2841 3202 0.1088 3203 0.1585 3204 0.0425 3205 0.0132 3206 0.0817 3207 0.0222 0901 0.0554 0902 0.0021 0903 0.0115 0904 0.0024 1801 0.0489 1802 0.0058 1803 0.0020 "],["estimación-fh-benchmark.html", "8.6 Estimación FH Benchmark", " 8.6 Estimación FH Benchmark El siguiente código realiza una serie de operaciones sobre dos data.frames: estimacionesPre y R_mpio para obtener las estimaciones ajustadas mediante el benchmarking. En primer lugar, se realiza un left_join entre estimacionesPre y R_mpio utilizando la columna id_dominio. Luego, se realiza un nuevo left_join entre este data.frame resultante y R_mpio2, utilizando la columna id_region. Este join agrega las columnas R_region_RB y R_region_DB de R_mpio2 al data.frame(). A continuación, se calcula una nueva columna llamada FH_RBench que resulta de multiplicar la columna FH por la columna R_region_RB. Por último, se realiza un join entre este data.frame resultante y pesos, utilizando la columna id_dominio. Este join agrega la columna W_i de pesos al data.frame. El resultado final se guarda en la variable estimacionesBench. estimacionesBench &lt;- estimacionesPre %&gt;% left_join(R_mpio %&gt;% dplyr::select(id_region, id_dominio), by = id_dominio) %&gt;% left_join(R_mpio2, by = c(&quot;id_region&quot;)) %&gt;% ungroup() %&gt;% mutate(FH_RBench = R_region_RB * FH) %&gt;% left_join(pesos, by = id_dominio) 8.6.1 Validación: Estimación FH con Benchmark Al realizar el proceso de validación notamos que las estimaciones con directas de las regiones y las estimaciones de FH con Benchmarking son iguales. estimacionesBench %&gt;% group_by(id_region) %&gt;% summarise(theta_reg_RB = sum(W_i * FH_RBench)) %&gt;% left_join(directoDepto, by = &quot;id_region&quot;) %&gt;% tba() id_region theta_reg_RB theta_region 01 0.4331 0.4331 02 0.5231 0.5231 03 0.5566 0.5566 04 0.4685 0.4685 8.6.2 Comparación gráfica entre estimación FH y FH con Benchmark por dominio El código presentado realiza una comparación de las estimaciones Fay-Herriot originales con las estimaciones Fay-Herriot con benchmark. temp_Bench &lt;- estimacionesBench %&gt;% transmute( id_dominio, Directo = Direct * 100, FayHerriot = FH * 100, FH_RBench = FH_RBench * 100, Gamma) theme_set(theme_bw()) ggplot(temp_Bench, aes(FH_RBench, FayHerriot)) + geom_point() + geom_abline(intercept = 0, slope = 1) + labs(y = &quot;Estimación Fay-Herriot&quot;, x = &quot;Estimación Fay-Herriot con Benchmark&quot;) "],["tabla-final-de-estimaciones.html", "8.7 Tabla final de estimaciones", " 8.7 Tabla final de estimaciones 8.7.1 Función para el calculo de intervalos de confianza La función ICL() sirve para calcular intervalos de confianza para la proporción de una población, dada una muestra y un nivel de confianza. Los argumentos de la función son: p: la proporción muestral mse: el error cuadrático medio alpha: el nivel de confianza deseado (por defecto es 0.05) student: un indicador booleano que indica si se debe usar una distribución t de Student en lugar de la distribución normal para el cálculo del intervalo de confianza (por defecto es FALSE) nu: el número de grados de libertad si se usa una distribución t de Student (por defecto es NULL y se calcula automáticamente) ICL &lt;- function(p, mse, alpha = 0.05, student = FALSE, nu = NULL) { if (student == TRUE) { q &lt;- qt(1 - alpha/2, nu) } else { q &lt;- qnorm(1 - alpha/2) } CL &lt;- log(p/(1 - p)) - (q * sqrt(mse))/(p * (1 - p)) CU &lt;- log(p/(1 - p)) + (q * sqrt(mse))/(p * (1 - p)) IC_1 &lt;- exp(CL)/(1 + exp(CL)) IC_2 &lt;- exp(CU)/(1 + exp(CU)) return(data.frame(L.I = IC_1, L.S = IC_2)) } La función TablaFinal toma como entrada la tabla estimacionesBench y realiza una serie de transformaciones y cálculos para generar una tabla final con las estimaciones y los intervalos de confianza correspondientes. Primero, agrega dos columnas nuevas: sintetico y sintetico_back. La columna sintetico se obtiene a partir de una matriz generada a partir de la columna base_FH de la tabla estimacionesBench, que contiene los valores de las variables auxiliares utilizadas en el modelo de Fay-Herriot. Esta matriz se multiplica por los coeficientes del modelo ajustado, que se asume que se ha generado previamente. La columna sintetico_back es el resultado de aplicar una transformación inversa al valor de sintetico. Luego, se seleccionan y renombran las columnas que se incluirán en la tabla final. Estas columnas incluyen la estimación directa (Directo), el error estándar de la estimación directa (ee_directo), el coeficiente de variación de la estimación directa (CV_directo), la estimación de Fay-Herriot (FayHerriot), la raíz del error cuadrático medio de la estimación de Fay-Herriot (rmse_FH), el coeficiente de variación de la estimación de Fay-Herriot (rrmse_FH), la estimación de gamma (Gamma), las estimaciones sintéticas (sintetico y sintetico_back), la estimación de Fay-Herriot con el ajuste de benchmark (FH_RBench), y los límites inferior y superior de los intervalos de confianza calculados utilizando el método de la distribución normal (LI_normal y LS_normal) y el método logit (LI_logit y LS_logit). Finalmente, se calculan los límites inferior y superior de los intervalos de confianza finales (LI_final y LS_final), teniendo en cuenta los límites de 0 y 1. TablaFinal &lt;- estimacionesBench %&gt;% mutate( sintetico = as.matrix(base_FH %&gt;% data.frame() %&gt;% dplyr::select(rownames( fh_arcsin$model$coefficients ))) %*% fh_arcsin$model$coefficients[, 1], sintetico_back = sin(sintetico) ^ 2 ) %&gt;% transmute( id_dominio, n_muestral = n, Directo = Direct, ee_directo = sqrt(Direct_MSE), CV_directo = Direct_CV, FayHerriot = FH, rmse_FH = sqrt(FH_MSE), rrmse_FH = rmse_FH / FayHerriot, Gamma, sintetico, sintetico_back, FH_RBench, LI_normal = FH_RBench - 1.96 * sqrt(FH_MSE), LS_normal = FH_RBench + 1.96 * sqrt(FH_MSE), LI_logit = ICL(FH_RBench, FH_MSE)[, 1], LS_logit = ICL(FH_RBench, FH_MSE)[, 2], LI_final = ifelse(LI_normal &lt; 0, LI_logit, LI_normal), LS_final = ifelse(LS_normal &gt; 1, LS_logit, LS_normal) ) tba(head(TablaFinal,20)) id_dominio n_muestral Directo ee_directo CV_directo FayHerriot rmse_FH rrmse_FH Gamma sintetico sintetico_back FH_RBench LI_normal LS_normal LI_logit LS_logit LI_final LS_final 0101 2951 0.4147 0.0221 0.0534 0.4142 0.0205 0.0496 0.1207 0.6990762 0.4141062 0.4118 0.3716 0.4521 0.3722 0.4526 0.3716 0.4521 0201 221 0.4526 0.0730 0.1613 0.5390 0.0413 0.0766 0.0128 0.8255541 0.5401128 0.5304 0.4494 0.6113 0.4493 0.6098 0.4494 0.6113 0202 86 NA NA NA 0.6466 0.0714 0.1104 NA 0.9341945 0.6466097 0.6362 0.4964 0.7761 0.4887 0.7620 0.4964 0.7761 0203 86 0.7138 0.1004 0.1406 0.7917 0.0540 0.0682 0.0056 1.0973752 0.7921244 0.7790 0.6732 0.8847 0.6560 0.8669 0.6732 0.8847 0204 51 NA NA NA 0.7472 0.0726 0.0972 NA 1.0439193 0.7471556 0.7352 0.5928 0.8775 0.5720 0.8522 0.5928 0.8775 0205 34 NA NA NA 0.7119 0.1019 0.1431 NA 1.0041934 0.7118791 0.7005 0.5008 0.9002 0.4745 0.8583 0.5008 0.9002 0206 65 0.5527 0.1154 0.2088 0.5870 0.0638 0.1087 0.0052 0.8730630 0.5872164 0.5776 0.4526 0.7027 0.4503 0.6954 0.4526 0.7027 0208 74 0.8122 0.0983 0.1210 0.6703 0.0775 0.1156 0.0044 0.9584522 0.6696196 0.6595 0.5077 0.8114 0.4962 0.7921 0.5077 0.8114 0210 16 NA NA NA 0.7081 0.1037 0.1465 NA 1.0000249 0.7080961 0.6967 0.4935 0.9000 0.4675 0.8574 0.4935 0.9000 0301 264 0.5668 0.0624 0.1100 0.5305 0.0362 0.0683 0.0170 0.8152491 0.5298332 0.5220 0.4509 0.5930 0.4510 0.5921 0.4509 0.5930 0302 123 0.7561 0.0769 0.1017 0.6841 0.0630 0.0921 0.0085 0.9733220 0.6835305 0.6732 0.5496 0.7967 0.5402 0.7832 0.5496 0.7967 0303 206 0.6078 0.0684 0.1125 0.6168 0.0353 0.0572 0.0139 0.9033845 0.6168944 0.6069 0.5377 0.6761 0.5360 0.6735 0.5377 0.6761 0304 176 0.6450 0.0716 0.1110 0.7021 0.0503 0.0717 0.0121 0.9942169 0.7028011 0.6908 0.5922 0.7895 0.5847 0.7800 0.5922 0.7895 0305 51 NA NA NA 0.6842 0.0729 0.1066 NA 0.9740382 0.6841966 0.6732 0.5303 0.8162 0.5183 0.7978 0.5303 0.8162 0401 481 0.5419 0.0455 0.0840 0.5609 0.0352 0.0627 0.0319 0.8471080 0.5615533 0.5519 0.4830 0.6209 0.4825 0.6194 0.4830 0.6209 0402 75 NA NA NA 0.7460 0.0684 0.0916 NA 1.0426203 0.7460256 0.7341 0.6001 0.8680 0.5815 0.8458 0.6001 0.8680 0403 108 0.6788 0.0913 0.1345 0.6959 0.0598 0.0859 0.0072 0.9868279 0.6960252 0.6847 0.5676 0.8019 0.5580 0.7889 0.5676 0.8019 0404 68 NA NA NA 0.5999 0.0754 0.1257 NA 0.8859597 0.5998849 0.5903 0.4425 0.7381 0.4388 0.7263 0.4425 0.7381 0405 221 0.5383 0.0704 0.1309 0.5777 0.0513 0.0889 0.0136 0.8639324 0.5782117 0.5684 0.4678 0.6690 0.4664 0.6649 0.4678 0.6690 0407 67 0.7513 0.1113 0.1482 0.6447 0.0677 0.1050 0.0042 0.9317482 0.6442693 0.6344 0.5017 0.7671 0.4948 0.7546 0.5017 0.7671 Comparando los limites superior e inferior de los IC a1 &lt;- ggplot(TablaFinal, aes(x = LI_normal, y = LI_logit)) + geom_point() + geom_abline(aes(intercept = 0, slope = 1), col = 2) + labs(y = &quot;LI_logit&quot;, x = &quot;LI&quot;) a2 &lt;- ggplot(TablaFinal, aes(x = LS_normal, y = LS_logit)) + geom_point() + geom_abline(aes(intercept = 0, slope = 1), col = 2) + labs(y = &quot;LS_logit&quot;, x = &quot;LS&quot;) a1 | a2 Guardar resultados saveRDS(TablaFinal, &#39;../Data/TablaFinal.Rds&#39;) saveRDS(estimacionesBench, &#39;../Data/estimacionesBench.Rds&#39;) "],["creando-mapa-de-resultados.html", "8.8 Creando mapa de resultados", " 8.8 Creando mapa de resultados La función tm_shape() de la librería tmap se utiliza para especificar los datos espaciales (mapa) que se van a graficar. En este caso, se le pasa como argumento la tabla poligonos_dominios. La función tm_fill() se utiliza para definir el estilo de relleno de los polígonos en el mapa. En este caso, se utiliza para graficar la variable fh_porc. Se utiliza el estilo “quantile” para que se dividan los valores en cuantiles y se apliquen diferentes tonalidades de color para cada cuantil. La función tm_borders() se utiliza para especificar las propiedades de las líneas que delinean los polígonos. La función tm_layout() se utiliza para definir los elementos del diseño del mapa, como la posición y estilo de la leyenda. En este caso, se define que la leyenda se ubique en la parte inferior central del mapa, que el fondo de la leyenda sea blanco con transparencia del 10%, y se especifica el tamaño del texto de la leyenda. poligonos_dominios &lt;- poligonos_dominios %&gt;% left_join( temp_Bench, by = id_dominio ) %&gt;% mutate( fh_porc = FH_RBench ) tmap_options(check.and.fix = TRUE) mapa &lt;- tm_shape( poligonos_dominios ) + tm_fill( &quot;fh_porc&quot;, style = &quot;quantile&quot;, title=&quot;Tasa de informalidad&quot; ) + tm_borders( col = &quot;black&quot;, lwd=1, lty = &quot;solid&quot;) + tm_layout( #&quot;Wealth (or so)&quot;, legend.title.size = 1, legend.text.size = 0.6, legend.position = c( &quot;center&quot;,&quot;bottom&quot; ), legend.bg.color = &quot;white&quot;, legend.stack = &quot;horizontal&quot;, #legend.digits = 5, legend.bg.alpha = 0.1) mapa "],["gráfico-comparativo-de-las-estimaciones..html", "Capítulo 9 Gráfico comparativo de las estimaciones. ", " Capítulo 9 Gráfico comparativo de las estimaciones. "],["lectura-de-librerías-5.html", "9.1 Lectura de librerías", " 9.1 Lectura de librerías library(plotly) library(dplyr) library(tidyr) library(forcats) library(survey) library(srvyr) "],["lecturas-de-bases-de-datos..html", "9.2 Lecturas de bases de datos.", " 9.2 Lecturas de bases de datos. El bloque de código carga tres archivos de datos en R: encuestaDOM.Rds: es utilizado para obtener las estimaciones directas de la región. estimacionesPre.Rds: contiene estimaciones previas, sin embargo solo son seleccionadas las columnas id_region, id_dominio y W_i que son útiles para la construcción del gráfico. TablaFinal.Rds: contiene las estimaciones directas, sintéticas, Fay Harriot y Fay Harriot con benchmarking. Primero se carga el archivo y luego se realiza una unión interna (inner_join()) con el archivo estimacionesPre por la variable id_dominio. encuestaDOM &lt;- readRDS(&quot;../Data/encuestaDOM.Rds&quot;) estimacionesPre &lt;- readRDS(&#39;../Data/estimacionesBench.Rds&#39;) %&gt;% dplyr::select(id_region, id_dominio, W_i) TablaFinal &lt;- readRDS(&#39;../Data/TablaFinal.Rds&#39;) %&gt;% inner_join(estimacionesPre, by = &quot;id_dominio&quot;) Este bloque de código calcula las estimaciones agregadas por región a partir de la tabla TablaFinal, que contiene las estimaciones sintéticas y de Fay Harriot para cada dominio, junto con su peso W_i. Primero, la tabla se agrupa por id_region. Luego, para cada región, se calcula la suma ponderada de las estimaciones sintéticas (sintetico_back) y de Fay Harriot (FH_RBench y FayHerriot) utilizando los pesos W_i. Las tres estimaciones se presentan en la tabla final estimaciones_agregada. estimaciones_agregada &lt;- TablaFinal %&gt;% data.frame() %&gt;% group_by(id_region) %&gt;% summarize( Sintetico=sum(W_i*sintetico_back), FH_bench=sum(W_i*FH_RBench), FH = sum(W_i*FayHerriot) ) tba(estimaciones_agregada) id_region Sintetico FH_bench FH 01 0.4355 0.4331 0.4355 02 0.5340 0.5231 0.5339 03 0.5656 0.5566 0.5656 04 0.4791 0.4685 0.4791 En este bloque de código se realizan las siguientes acciones: Se agrega ceros a la izquierda de las variables upm y estrato para que tengan una longitud de 9 y 5 dígitos, respectivamente. Se crea un objeto disenoDOM con el diseño de la encuesta, utilizando la función as_survey_design() de la librería survey. Se especifica que el estrato es la variable estrato, la unidad primaria de muestreo (UPM) es la variable upm y el peso muestral es la variable factor_anual. Además, se utiliza el argumento nest=T para indicar que las UPM están anidadas. Se estima el indicador directo para cada región. Para ello, se agrupan id_region y se utiliza la función summarise() para calcular el tamaño de la muestra (n) y la razón de diseño (Rd) utilizando la función survey_ratio() de la librería survey. encuestaDOM &lt;- encuestaDOM %&gt;% mutate( upm = str_pad(string = upm,width = 9,pad = &quot;0&quot;), estrato = str_pad(string = estrato,width = 5,pad = &quot;0&quot;), factor_anual = factor_expansion / 4 ) #Creación de objeto diseno--------------------------------------- disenoDOM &lt;- encuestaDOM %&gt;% as_survey_design( strata = estrato, ids = upm, weights = factor_anual, nest=T ) #Cálculo del indicador ---------------- indicador_dir &lt;- disenoDOM %&gt;% group_by(id_region) %&gt;% filter(ocupado == 1 &amp; pet == 1) %&gt;% summarise( n = unweighted(n()), Rd = survey_ratio( numerator = orden_sector == 2 , denominator = 1, vartype = c(&quot;se&quot;, &quot;ci&quot;), deff = F ) ) Este bloque de código realiza lo siguiente: Primero, se une el objeto estimaciones_agregada que contiene las estimaciones de los métodos sintético, FH benchmark y FH, con el objeto indicador_dir que contiene el indicador directo para cada región. Luego, se seleccionan las columnas necesarias para el gráfico y se transforma el formato de las columnas usando la funcióngather() para tener los valores de las estimaciones en una sola columna y el nombre del método en otra columna. Además, se cambia el nombre de los métodos para que sean más descriptivos. Finalmente, se crean los límites del intervalo de confianza para el indicador directo y se guardan en el objeto lims_IC. data_plot &lt;- left_join(estimaciones_agregada, indicador_dir, by = &#39;id_region&#39;) %&gt;% data.frame() temp &lt;- data_plot %&gt;% select(-Rd_low, -Rd_upp,-Rd_se) %&gt;% gather(key = &quot;Estimacion&quot;,value = &quot;value&quot;, -n,-id_region) %&gt;% mutate(Estimacion = case_when(Estimacion == &quot;FH&quot; ~ &quot;Fay Harriot&quot;, Estimacion == &quot;FH_bench&quot; ~ &quot;FH bench&quot;, Estimacion == &quot;Rd&quot;~ &quot;Directo&quot;, TRUE ~ Estimacion)) lims_IC &lt;- data_plot %&gt;% select(n,id_region,value = Rd, Rd_low, Rd_upp) %&gt;% mutate(Estimacion = &quot;Directo&quot;) Este bloque de código grafica las estimaciones de los diferentes métodos, junto con los intervalos de confianza de las estimaciones directas. La funciónggplot() de la librería ggplot2 se utiliza para crear el gráfico, y se utilizan diferentes capas para agregar diferentes elementos al gráfico. p &lt;- ggplot(temp, aes( x = fct_reorder2(id_region, id_region, n), y = value, shape = Estimacion, color = Estimacion )) + geom_errorbar( data = lims_IC, aes(ymin = Rd_low , ymax = Rd_upp, x = id_region), width = 0.2, size = 1 ) + geom_jitter(size = 3)+ labs(x = &quot;Región&quot;) ggplotly(p) La capa aes() define la estética de la gráfica. En este caso, se utiliza la variable id_region en el eje x, la variable value en el eje y, y se utiliza el color y la forma de los puntos para distinguir entre los diferentes métodos de estimación. Se utiliza la función fct_reorder2() para ordenar las regiones de manera ascendente según su tamaño muestral n. La capa geom_errorbar() se utiliza para agregar los intervalos de confianza de las estimaciones directas. Se utiliza el conjunto de datos lims_IC que contiene los límites de los intervalos de confianza. Se utiliza ymin y ymax para definir los límites inferior y superior de las barras de error, respectivamente, y x para definir la ubicación en el eje x de cada intervalo. La capa geom_jitter() se utiliza para agregar los puntos de las estimaciones de cada método, con un poco de ruido para que no se superpongan. Finalmente, la capa labs() se utiliza para agregar etiquetas a los ejes del gráfico. En este caso, se utiliza Región para el eje x. El objeto p contiene el gráfico creado. "],["resultados-finales.html", "Capítulo 10 Resultados finales", " Capítulo 10 Resultados finales Codico del municipio Estimacion Error estandar CV(%) 3201 0.4191 0.0197 4.6813 0601 0.5803 0.0286 4.8231 0101 0.4118 0.0205 4.9604 2902 0.6007 0.0306 4.9792 0604 0.7061 0.0361 5.0126 2101 0.4601 0.0238 5.0894 2501 0.4045 0.0218 5.2862 3203 0.4911 0.0262 5.3144 0901 0.4940 0.0282 5.5985 1301 0.4825 0.0280 5.6759 0303 0.6069 0.0353 5.7247 2201 0.5756 0.0338 5.7837 1401 0.6128 0.0363 5.8071 3202 0.4018 0.0237 5.8530 1801 0.4920 0.0299 5.9545 0903 0.5912 0.0362 5.9966 3204 0.5210 0.0317 6.0463 2205 0.6241 0.0386 6.0891 3101 0.6474 0.0405 6.1558 2801 0.5660 0.0357 6.1716 1809 0.5407 0.0342 6.1973 0401 0.5519 0.0352 6.2693 2507 0.5230 0.0345 6.4628 1403 0.6948 0.0464 6.5464 2701 0.5550 0.0372 6.5591 3001 0.5319 0.0357 6.5605 2402 0.6418 0.0445 6.8000 3207 0.5537 0.0379 6.8072 0203 0.7790 0.0540 6.8164 0301 0.5220 0.0362 6.8324 1303 0.5712 0.0405 6.9534 0602 0.6824 0.0485 6.9648 2301 0.4703 0.0339 7.0455 2803 0.6341 0.0463 7.1557 0304 0.6908 0.0503 7.1690 1903 0.6294 0.0470 7.3152 2502 0.5207 0.0395 7.4240 2104 0.5942 0.0450 7.4516 0607 0.7440 0.0570 7.5098 2901 0.5697 0.0445 7.6338 0201 0.5304 0.0413 7.6649 2203 0.7384 0.0576 7.6702 2105 0.5204 0.0411 7.7788 1302 0.5569 0.0445 7.8213 1701 0.5159 0.0410 7.8224 2404 0.6179 0.0494 7.8341 0802 0.5935 0.0476 7.8415 2401 0.5233 0.0421 7.8830 3206 0.4019 0.0319 7.8988 1808 0.6207 0.0503 7.9337 1802 0.6460 0.0530 8.0387 2002 0.6042 0.0496 8.0415 1101 0.3579 0.0295 8.0516 2001 0.6262 0.0526 8.2306 2506 0.4441 0.0375 8.2796 2903 0.5864 0.0499 8.3275 2202 0.7496 0.0634 8.3277 2904 0.5980 0.0510 8.3437 1203 0.5434 0.0464 8.3509 0904 0.5929 0.0507 8.3852 3003 0.7079 0.0607 8.3883 1404 0.6815 0.0587 8.4406 1805 0.6977 0.0607 8.5232 0403 0.6847 0.0598 8.5881 1402 0.5864 0.0526 8.7892 2304 0.4873 0.0440 8.8239 1001 0.5727 0.0514 8.8394 2503 0.6934 0.0629 8.8843 0405 0.5684 0.0513 8.8851 2403 0.6014 0.0548 8.9258 1503 0.5498 0.0501 8.9318 1504 0.6833 0.0630 9.0315 1505 0.7065 0.0654 9.0654 0402 0.7341 0.0684 9.1628 2905 0.7170 0.0675 9.2020 0302 0.6732 0.0630 9.2143 0406 0.7824 0.0738 9.2856 3205 0.4671 0.0441 9.3953 0503 0.6934 0.0667 9.4203 2702 0.5450 0.0526 9.4588 0704 0.8148 0.0785 9.4735 3103 0.8061 0.0781 9.5311 1807 0.5279 0.0518 9.6104 1902 0.6461 0.0640 9.7082 0204 0.7352 0.0726 9.7218 3002 0.6026 0.0606 9.8296 2305 0.5244 0.0527 9.8332 2504 0.5118 0.0515 9.8617 1901 0.5399 0.0546 9.9006 2103 0.4002 0.0404 9.9262 2601 0.6008 0.0611 9.9594 2703 0.6194 0.0631 9.9876 2603 0.6122 0.0625 9.9990 3102 0.7093 0.0725 10.0564 1201 0.3941 0.0409 10.1563 0701 0.6123 0.0634 10.1962 2802 0.4911 0.0515 10.2721 1806 0.5701 0.0601 10.3223 1501 0.5392 0.0570 10.3570 0408 0.7528 0.0793 10.3658 1702 0.5278 0.0559 10.4131 0603 0.6444 0.0690 10.4884 0801 0.4319 0.0464 10.4972 0407 0.6344 0.0677 10.4994 2107 0.4417 0.0476 10.6130 0305 0.6732 0.0729 10.6587 2102 0.5302 0.0575 10.6641 0606 0.6388 0.0696 10.6788 0501 0.5157 0.0563 10.6996 0902 0.5921 0.0655 10.8437 0206 0.5776 0.0638 10.8699 1804 0.5700 0.0634 10.9026 0605 0.6672 0.0747 10.9664 0202 0.6362 0.0714 11.0369 2508 0.4093 0.0462 11.0654 1102 0.4651 0.0531 11.1586 2106 0.4415 0.0514 11.4504 1304 0.4789 0.0563 11.5236 0208 0.6595 0.0775 11.5607 0502 0.5588 0.0671 11.7577 2206 0.8007 0.0965 11.8583 1601 0.5702 0.0687 11.8589 1506 0.5548 0.0676 11.9359 1502 0.6285 0.0772 12.0334 1002 0.5425 0.0667 12.1009 2303 0.5484 0.0685 12.2159 0411 0.7268 0.0903 12.2206 0207 0.6864 0.0868 12.4458 0404 0.5903 0.0754 12.5699 0409 0.7034 0.0904 12.6515 2505 0.5465 0.0715 12.8127 0209 0.7456 0.0986 13.0068 1803 0.5815 0.0788 13.2856 1003 0.5552 0.0785 13.9060 2602 0.6421 0.0929 14.1715 2509 0.5278 0.0764 14.1854 0205 0.7005 0.1019 14.3111 0702 0.7351 0.1080 14.4632 0210 0.6967 0.1037 14.6450 1005 0.5748 0.0856 14.6475 0706 0.7822 0.1179 14.8284 2204 0.6252 0.0947 14.8985 2302 0.4087 0.0652 15.6126 0504 0.5568 0.0899 15.8200 2306 0.4035 0.0655 15.8646 1602 0.6280 0.1020 15.9795 1202 0.3644 0.0608 16.3099 2003 0.6520 0.1086 16.3151 0410 0.5266 0.0886 16.5560 1004 0.5422 0.1032 18.7286 2108 0.6988 0.1367 19.2503 0705 0.6216 0.1232 19.4951 0703 0.6370 0.1347 20.8065 1006 0.5964 0.1340 22.1069 0505 0.5411 0.1256 22.7386 "]]
